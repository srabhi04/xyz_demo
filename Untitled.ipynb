{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMpM8iiSnfY7"
      },
      "outputs": [],
      "source": [
        "# üöÄ SPARK SETUP FOR GOOGLE COLAB (Stable & Tested)\n",
        "\n",
        "# 1Ô∏è‚É£ Install Java, Spark, and Findspark\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark pyspark\n",
        "\n",
        "# 2Ô∏è‚É£ Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
        "os.environ[\"PATH\"] += \":/content/spark-3.5.0-bin-hadoop3/bin\"\n",
        "\n",
        "# 3Ô∏è‚É£ Initialize Spark\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-3.5.0-bin-hadoop3\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark_Practicals_Colab\") \\\n",
        "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 4Ô∏è‚É£ Import Common Libraries\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "print(\"‚úÖ Spark Setup Complete! Version:\", spark.version)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# 1. Parallelize (create RDD from a Python list)\n",
        "# ------------------------------------------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RDD_Practical\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(\"SparkContext initialized:\", sc)\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6]\n",
        "rdd1 = sc.parallelize(data)\n",
        "print(\"Parallelized RDD:\", rdd1.collect())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. Read Text File into RDD\n",
        "# ------------------------------------------------------------\n",
        "# Create a sample text file\n",
        "with open(\"sample_text.txt\", \"w\") as f:\n",
        "    f.write(\"Hello Spark\\n\")\n",
        "    f.write(\"RDD Basics\\n\")\n",
        "    f.write(\"PySpark in Colab\\n\")\n",
        "\n",
        "text_rdd = sc.textFile(\"sample_text.txt\")\n",
        "print(\"Text File RDD:\", text_rdd.collect())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. Read Multiple Text Files into RDD\n",
        "# ------------------------------------------------------------\n",
        "# Create more text files\n",
        "with open(\"text1.txt\", \"w\") as f:\n",
        "    f.write(\"File One\\nLine A\\nLine B\\n\")\n",
        "with open(\"text2.txt\", \"w\") as f:\n",
        "    f.write(\"File Two\\nLine C\\nLine D\\n\")\n",
        "\n",
        "multi_text_rdd = sc.textFile(\"text*.txt\")\n",
        "print(\"Multiple Text Files RDD:\", multi_text_rdd.collect())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. Read CSV File into RDD\n",
        "# ------------------------------------------------------------\n",
        "# Create a sample CSV file\n",
        "with open(\"sample.csv\", \"w\") as f:\n",
        "    f.write(\"id,name,age\\n\")\n",
        "    f.write(\"1,Alice,23\\n\")\n",
        "    f.write(\"2,Bob,30\\n\")\n",
        "    f.write(\"3,Charlie,28\\n\")\n",
        "\n",
        "csv_rdd = sc.textFile(\"sample.csv\")\n",
        "csv_parsed = csv_rdd.map(lambda line: line.split(\",\"))\n",
        "print(\"CSV RDD:\", csv_parsed.collect())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. Create Empty RDD\n",
        "# ------------------------------------------------------------\n",
        "empty_rdd = sc.emptyRDD()\n",
        "print(\"Empty RDD Count:\", empty_rdd.count())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. RDD Actions\n",
        "# ------------------------------------------------------------\n",
        "print(\"Count:\", rdd1.count())\n",
        "print(\"First Element:\", rdd1.first())\n",
        "print(\"Sum:\", rdd1.sum())\n",
        "print(\"Take 3:\", rdd1.take(3))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 7. Pair RDD Functions (key-value pairs)\n",
        "# ------------------------------------------------------------\n",
        "pair_rdd = rdd1.map(lambda x: (x, x*2))\n",
        "print(\"Pair RDD:\", pair_rdd.collect())\n",
        "\n",
        "# Example: word count\n",
        "word_rdd = sc.parallelize([\"hello world\", \"hello spark\", \"spark rdd\"])\n",
        "word_pairs = word_rdd.flatMap(lambda line: line.split(\" \")) \\\n",
        "                     .map(lambda word: (word, 1)) \\\n",
        "                     .reduceByKey(lambda a,b: a+b)\n",
        "print(\"Word Count:\", word_pairs.collect())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 8. Repartition and Coalesce\n",
        "# ------------------------------------------------------------\n",
        "print(\"Original Partitions:\", rdd1.getNumPartitions())\n",
        "rdd_repart = rdd1.repartition(4)\n",
        "print(\"Repartitioned Partitions:\", rdd_repart.getNumPartitions())\n",
        "rdd_coalesce = rdd_repart.coalesce(2)\n",
        "print(\"Coalesced Partitions:\", rdd_coalesce.getNumPartitions())\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 9. Shuffle Partitions (configure shuffle)\n",
        "# ------------------------------------------------------------\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
        "print(\"Shuffle Partitions set to:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 10. Broadcast Variables\n",
        "# ------------------------------------------------------------\n",
        "broadcast_var = sc.broadcast([10,20,30])\n",
        "print(\"Broadcast Value:\", broadcast_var.value)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 11. Accumulator Variables\n",
        "# ------------------------------------------------------------\n",
        "accum = sc.accumulator(0)\n",
        "\n",
        "def add_accum(x):\n",
        "    global accum\n",
        "    accum += x\n",
        "\n",
        "rdd1.foreach(add_accum)\n",
        "print(\"Accumulator Sum:\", accum.value)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 12. Convert RDD to DataFrame\n",
        "# ------------------------------------------------------------\n",
        "df_from_rdd = csv_parsed.toDF([\"id\", \"name\", \"age\"])\n",
        "df_from_rdd.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Htb0OkzngVb",
        "outputId": "8c97b415-c038-4710-b2a8-4efdc76ec998"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkContext initialized: <SparkContext master=local[*] appName=Spark_Practicals_Colab>\n",
            "Parallelized RDD: [1, 2, 3, 4, 5, 6]\n",
            "Text File RDD: ['Hello Spark', 'RDD Basics', 'PySpark in Colab']\n",
            "Multiple Text Files RDD: ['File One', 'Line A', 'Line B', 'File Two', 'Line C', 'Line D']\n",
            "CSV RDD: [['id', 'name', 'age'], ['1', 'Alice', '23'], ['2', 'Bob', '30'], ['3', 'Charlie', '28']]\n",
            "Empty RDD Count: 0\n",
            "Count: 6\n",
            "First Element: 1\n",
            "Sum: 21\n",
            "Take 3: [1, 2, 3]\n",
            "Pair RDD: [(1, 2), (2, 4), (3, 6), (4, 8), (5, 10), (6, 12)]\n",
            "Word Count: [('hello', 2), ('world', 1), ('rdd', 1), ('spark', 2)]\n",
            "Original Partitions: 2\n",
            "Repartitioned Partitions: 4\n",
            "Coalesced Partitions: 2\n",
            "Shuffle Partitions set to: 5\n",
            "Broadcast Value: [10, 20, 30]\n",
            "Accumulator Sum: 21\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "|  1|  Alice| 23|\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 28|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Practical2_DataFrameOps\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# 1. Create an empty DataFrame\n",
        "empty_df = spark.createDataFrame([], StructType([]))\n",
        "print(\"Empty DataFrame:\")\n",
        "empty_df.show()\n",
        "\n",
        "# 2. Create an empty Dataset (in PySpark = typed DataFrame)\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True)\n",
        "])\n",
        "empty_ds = spark.createDataFrame([], schema)\n",
        "print(\"Empty Dataset (typed DataFrame):\")\n",
        "empty_ds.show()\n",
        "\n",
        "# Create sample DataFrame for next operations\n",
        "data = [\n",
        "    (1, \"Alice\", 2000, \"HR\"),\n",
        "    (2, \"Bob\", 2500, \"IT\"),\n",
        "    (3, \"Cathy\", 3000, \"IT\"),\n",
        "    (4, \"David\", None, \"Finance\"),\n",
        "    (5, \"Eve\", 2800, \"Finance\"),\n",
        "    (6, \"Frank\", None, \"HR\")\n",
        "]\n",
        "columns = [\"id\", \"name\", \"salary\", \"dept\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# 3. Rename nested column (simulate nested structure)\n",
        "nested_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"info\", StructType([\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"salary\", IntegerType(), True)\n",
        "    ]))\n",
        "])\n",
        "nested_data = [(1, (\"Alice\", 2000)), (2, (\"Bob\", 2500))]\n",
        "nested_df = spark.createDataFrame(nested_data, nested_schema)\n",
        "renamed_df = nested_df.withColumnRenamed(\"info.name\", \"employee_name\")\n",
        "print(\"Rename nested column:\")\n",
        "renamed_df.show()\n",
        "\n",
        "# 4. Add or Update a column\n",
        "df = df.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
        "print(\"Added Bonus column:\")\n",
        "df.show()\n",
        "\n",
        "# 5. Drop a column\n",
        "df_drop = df.drop(\"bonus\")\n",
        "print(\"After Dropping Bonus column:\")\n",
        "df_drop.show()\n",
        "\n",
        "# 6. Add literal constant\n",
        "df_lit = df.withColumn(\"Country\", lit(\"India\"))\n",
        "print(\"Added constant column:\")\n",
        "df_lit.show()\n",
        "\n",
        "# 7. Change column data type\n",
        "df_cast = df.withColumn(\"salary\", col(\"salary\").cast(\"double\"))\n",
        "print(\"Salary converted to double:\")\n",
        "df_cast.show()\n",
        "\n",
        "# 8. Pivot & Unpivot\n",
        "sales_data = [\n",
        "    (\"Q1\", \"ProductA\", 100),\n",
        "    (\"Q1\", \"ProductB\", 150),\n",
        "    (\"Q2\", \"ProductA\", 200),\n",
        "    (\"Q2\", \"ProductB\", 250),\n",
        "]\n",
        "sales_df = spark.createDataFrame(sales_data, [\"quarter\", \"product\", \"revenue\"])\n",
        "\n",
        "pivot_df = sales_df.groupBy(\"quarter\").pivot(\"product\").sum(\"revenue\")\n",
        "print(\"Pivot Example:\")\n",
        "pivot_df.show()\n",
        "\n",
        "unpivot_df = pivot_df.selectExpr(\"quarter\", \"stack(2, 'ProductA', ProductA, 'ProductB', ProductB) as (product, revenue)\")\n",
        "print(\"Unpivot Example:\")\n",
        "unpivot_df.show()\n",
        "\n",
        "# 9. Create DataFrame using StructType & StructField\n",
        "custom_schema = StructType([\n",
        "    StructField(\"emp_id\", IntegerType(), True),\n",
        "    StructField(\"emp_name\", StringType(), True),\n",
        "    StructField(\"emp_salary\", DoubleType(), True)\n",
        "])\n",
        "custom_data = [(101, \"John\", 5000.0), (102, \"Mike\", 6000.0)]\n",
        "custom_df = spark.createDataFrame(custom_data, custom_schema)\n",
        "print(\"Custom schema DataFrame:\")\n",
        "custom_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etWq353M8W2j",
        "outputId": "05636637-55eb-485f-ae0d-0fa27792c602"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame:\n",
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n",
            "Empty Dataset (typed DataFrame):\n",
            "+---+----+\n",
            "| id|name|\n",
            "+---+----+\n",
            "+---+----+\n",
            "\n",
            "Rename nested column:\n",
            "+---+-------------+\n",
            "| id|         info|\n",
            "+---+-------------+\n",
            "|  1|{Alice, 2000}|\n",
            "|  2|  {Bob, 2500}|\n",
            "+---+-------------+\n",
            "\n",
            "Added Bonus column:\n",
            "+---+-----+------+-------+-----+\n",
            "| id| name|salary|   dept|bonus|\n",
            "+---+-----+------+-------+-----+\n",
            "|  1|Alice|  2000|     HR|200.0|\n",
            "|  2|  Bob|  2500|     IT|250.0|\n",
            "|  3|Cathy|  3000|     IT|300.0|\n",
            "|  4|David|  NULL|Finance| NULL|\n",
            "|  5|  Eve|  2800|Finance|280.0|\n",
            "|  6|Frank|  NULL|     HR| NULL|\n",
            "+---+-----+------+-------+-----+\n",
            "\n",
            "After Dropping Bonus column:\n",
            "+---+-----+------+-------+\n",
            "| id| name|salary|   dept|\n",
            "+---+-----+------+-------+\n",
            "|  1|Alice|  2000|     HR|\n",
            "|  2|  Bob|  2500|     IT|\n",
            "|  3|Cathy|  3000|     IT|\n",
            "|  4|David|  NULL|Finance|\n",
            "|  5|  Eve|  2800|Finance|\n",
            "|  6|Frank|  NULL|     HR|\n",
            "+---+-----+------+-------+\n",
            "\n",
            "Added constant column:\n",
            "+---+-----+------+-------+-----+-------+\n",
            "| id| name|salary|   dept|bonus|Country|\n",
            "+---+-----+------+-------+-----+-------+\n",
            "|  1|Alice|  2000|     HR|200.0|  India|\n",
            "|  2|  Bob|  2500|     IT|250.0|  India|\n",
            "|  3|Cathy|  3000|     IT|300.0|  India|\n",
            "|  4|David|  NULL|Finance| NULL|  India|\n",
            "|  5|  Eve|  2800|Finance|280.0|  India|\n",
            "|  6|Frank|  NULL|     HR| NULL|  India|\n",
            "+---+-----+------+-------+-----+-------+\n",
            "\n",
            "Salary converted to double:\n",
            "+---+-----+------+-------+-----+\n",
            "| id| name|salary|   dept|bonus|\n",
            "+---+-----+------+-------+-----+\n",
            "|  1|Alice|2000.0|     HR|200.0|\n",
            "|  2|  Bob|2500.0|     IT|250.0|\n",
            "|  3|Cathy|3000.0|     IT|300.0|\n",
            "|  4|David|  NULL|Finance| NULL|\n",
            "|  5|  Eve|2800.0|Finance|280.0|\n",
            "|  6|Frank|  NULL|     HR| NULL|\n",
            "+---+-----+------+-------+-----+\n",
            "\n",
            "Pivot Example:\n",
            "+-------+--------+--------+\n",
            "|quarter|ProductA|ProductB|\n",
            "+-------+--------+--------+\n",
            "|     Q1|     100|     150|\n",
            "|     Q2|     200|     250|\n",
            "+-------+--------+--------+\n",
            "\n",
            "Unpivot Example:\n",
            "+-------+--------+-------+\n",
            "|quarter| product|revenue|\n",
            "+-------+--------+-------+\n",
            "|     Q1|ProductA|    100|\n",
            "|     Q1|ProductB|    150|\n",
            "|     Q2|ProductA|    200|\n",
            "|     Q2|ProductB|    250|\n",
            "+-------+--------+-------+\n",
            "\n",
            "Custom schema DataFrame:\n",
            "+------+--------+----------+\n",
            "|emp_id|emp_name|emp_salary|\n",
            "+------+--------+----------+\n",
            "|   101|    John|    5000.0|\n",
            "|   102|    Mike|    6000.0|\n",
            "+------+--------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Selecting the first row of each group\n",
        "first_row = df.groupBy(\"dept\").agg(first(\"name\").alias(\"first_employee\"))\n",
        "print(\"First row of each group:\")\n",
        "first_row.show()\n",
        "\n",
        "# 2. Sort DataFrame\n",
        "df_sorted = df.orderBy(col(\"salary\").desc_nulls_last())\n",
        "print(\"Sorted DataFrame:\")\n",
        "df_sorted.show()\n",
        "\n",
        "# 3. Union DataFrame\n",
        "df_union = df.union(df)\n",
        "print(\"Union DataFrame:\")\n",
        "df_union.show()\n",
        "\n",
        "# 4. Drop rows with null values\n",
        "df_dropna = df.na.drop()\n",
        "print(\"Drop null rows:\")\n",
        "df_dropna.show()\n",
        "\n",
        "# 5. Split single column into multiple\n",
        "split_df = df.withColumn(\"name_split\", split(col(\"name\"), \"a\"))\n",
        "print(\"Split name column:\")\n",
        "split_df.show()\n",
        "\n",
        "# 6. Concatenate multiple columns\n",
        "concat_df = df.withColumn(\"full_info\", concat_ws(\"-\", col(\"name\"), col(\"dept\")))\n",
        "print(\"Concatenate columns:\")\n",
        "concat_df.show()\n",
        "\n",
        "# 7. Replace null values\n",
        "df_fill = df.na.fill({\"salary\": 0, \"name\": \"Unknown\"})\n",
        "print(\"Fill nulls:\")\n",
        "df_fill.show()\n",
        "\n",
        "# 8. Remove duplicate rows\n",
        "df_nodup = df.dropDuplicates()\n",
        "print(\"Removed duplicates:\")\n",
        "df_nodup.show()\n",
        "\n",
        "# 9. Distinct on multiple selected columns\n",
        "df_distinct = df.select(\"dept\", \"salary\").distinct()\n",
        "print(\"Distinct on dept & salary:\")\n",
        "df_distinct.show()\n",
        "\n",
        "# 10. Spark UDF\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def upper_case(name):\n",
        "    return name.upper()\n",
        "\n",
        "upper_udf = udf(upper_case, StringType())\n",
        "df_udf = df.withColumn(\"name_upper\", upper_udf(col(\"name\")))\n",
        "print(\"Using UDF:\")\n",
        "df_udf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAkqPOcU8rI1",
        "outputId": "0548d26a-3544-49e9-fef5-685f5fa3d998"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row of each group:\n",
            "+-------+--------------+\n",
            "|   dept|first_employee|\n",
            "+-------+--------------+\n",
            "|Finance|         David|\n",
            "|     HR|         Alice|\n",
            "|     IT|           Bob|\n",
            "+-------+--------------+\n",
            "\n",
            "Sorted DataFrame:\n",
            "+---+-----+------+-------+-----+\n",
            "| id| name|salary|   dept|bonus|\n",
            "+---+-----+------+-------+-----+\n",
            "|  3|Cathy|  3000|     IT|300.0|\n",
            "|  5|  Eve|  2800|Finance|280.0|\n",
            "|  2|  Bob|  2500|     IT|250.0|\n",
            "|  1|Alice|  2000|     HR|200.0|\n",
            "|  4|David|  NULL|Finance| NULL|\n",
            "|  6|Frank|  NULL|     HR| NULL|\n",
            "+---+-----+------+-------+-----+\n",
            "\n",
            "Union DataFrame:\n",
            "+---+-----+------+-------+-----+\n",
            "| id| name|salary|   dept|bonus|\n",
            "+---+-----+------+-------+-----+\n",
            "|  1|Alice|  2000|     HR|200.0|\n",
            "|  2|  Bob|  2500|     IT|250.0|\n",
            "|  3|Cathy|  3000|     IT|300.0|\n",
            "|  4|David|  NULL|Finance| NULL|\n",
            "|  5|  Eve|  2800|Finance|280.0|\n",
            "|  6|Frank|  NULL|     HR| NULL|\n",
            "|  1|Alice|  2000|     HR|200.0|\n",
            "|  2|  Bob|  2500|     IT|250.0|\n",
            "|  3|Cathy|  3000|     IT|300.0|\n",
            "|  4|David|  NULL|Finance| NULL|\n",
            "|  5|  Eve|  2800|Finance|280.0|\n",
            "|  6|Frank|  NULL|     HR| NULL|\n",
            "+---+-----+------+-------+-----+\n",
            "\n",
            "Drop null rows:\n",
            "+---+-----+------+-------+-----+\n",
            "| id| name|salary|   dept|bonus|\n",
            "+---+-----+------+-------+-----+\n",
            "|  1|Alice|  2000|     HR|200.0|\n",
            "|  2|  Bob|  2500|     IT|250.0|\n",
            "|  3|Cathy|  3000|     IT|300.0|\n",
            "|  5|  Eve|  2800|Finance|280.0|\n",
            "+---+-----+------+-------+-----+\n",
            "\n",
            "Split name column:\n",
            "+---+-----+------+-------+-----+----------+\n",
            "| id| name|salary|   dept|bonus|name_split|\n",
            "+---+-----+------+-------+-----+----------+\n",
            "|  1|Alice|  2000|     HR|200.0|   [Alice]|\n",
            "|  2|  Bob|  2500|     IT|250.0|     [Bob]|\n",
            "|  3|Cathy|  3000|     IT|300.0|  [C, thy]|\n",
            "|  4|David|  NULL|Finance| NULL|  [D, vid]|\n",
            "|  5|  Eve|  2800|Finance|280.0|     [Eve]|\n",
            "|  6|Frank|  NULL|     HR| NULL|  [Fr, nk]|\n",
            "+---+-----+------+-------+-----+----------+\n",
            "\n",
            "Concatenate columns:\n",
            "+---+-----+------+-------+-----+-------------+\n",
            "| id| name|salary|   dept|bonus|    full_info|\n",
            "+---+-----+------+-------+-----+-------------+\n",
            "|  1|Alice|  2000|     HR|200.0|     Alice-HR|\n",
            "|  2|  Bob|  2500|     IT|250.0|       Bob-IT|\n",
            "|  3|Cathy|  3000|     IT|300.0|     Cathy-IT|\n",
            "|  4|David|  NULL|Finance| NULL|David-Finance|\n",
            "|  5|  Eve|  2800|Finance|280.0|  Eve-Finance|\n",
            "|  6|Frank|  NULL|     HR| NULL|     Frank-HR|\n",
            "+---+-----+------+-------+-----+-------------+\n",
            "\n",
            "Fill nulls:\n",
            "+---+-----+------+-------+-----+\n",
            "| id| name|salary|   dept|bonus|\n",
            "+---+-----+------+-------+-----+\n",
            "|  1|Alice|  2000|     HR|200.0|\n",
            "|  2|  Bob|  2500|     IT|250.0|\n",
            "|  3|Cathy|  3000|     IT|300.0|\n",
            "|  4|David|     0|Finance| NULL|\n",
            "|  5|  Eve|  2800|Finance|280.0|\n",
            "|  6|Frank|     0|     HR| NULL|\n",
            "+---+-----+------+-------+-----+\n",
            "\n",
            "Removed duplicates:\n",
            "+---+-----+------+-------+-----+\n",
            "| id| name|salary|   dept|bonus|\n",
            "+---+-----+------+-------+-----+\n",
            "|  1|Alice|  2000|     HR|200.0|\n",
            "|  2|  Bob|  2500|     IT|250.0|\n",
            "|  3|Cathy|  3000|     IT|300.0|\n",
            "|  5|  Eve|  2800|Finance|280.0|\n",
            "|  4|David|  NULL|Finance| NULL|\n",
            "|  6|Frank|  NULL|     HR| NULL|\n",
            "+---+-----+------+-------+-----+\n",
            "\n",
            "Distinct on dept & salary:\n",
            "+-------+------+\n",
            "|   dept|salary|\n",
            "+-------+------+\n",
            "|     IT|  3000|\n",
            "|     HR|  2000|\n",
            "|     IT|  2500|\n",
            "|Finance|  2800|\n",
            "|Finance|  NULL|\n",
            "|     HR|  NULL|\n",
            "+-------+------+\n",
            "\n",
            "Using UDF:\n",
            "+---+-----+------+-------+-----+----------+\n",
            "| id| name|salary|   dept|bonus|name_upper|\n",
            "+---+-----+------+-------+-----+----------+\n",
            "|  1|Alice|  2000|     HR|200.0|     ALICE|\n",
            "|  2|  Bob|  2500|     IT|250.0|       BOB|\n",
            "|  3|Cathy|  3000|     IT|300.0|     CATHY|\n",
            "|  4|David|  NULL|Finance| NULL|     DAVID|\n",
            "|  5|  Eve|  2800|Finance|280.0|       EVE|\n",
            "|  6|Frank|  NULL|     HR| NULL|     FRANK|\n",
            "+---+-----+------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Practical3_ArrayMap\").getOrCreate()\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Array_Map_Operations\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30)]\n",
        "df = spark.createDataFrame(data, [\"name\",\"age\"])\n",
        "\n",
        "# Add Array column\n",
        "df = df.withColumn(\"scores\", array(lit(85), lit(90), lit(95)))\n",
        "df.show(truncate=False)\n",
        "df = df.withColumn(\"subject_scores\", map_from_arrays(array(lit(\"Math\"), lit(\"Physics\")),array(lit(85), lit(90))))\n",
        "df.show(truncate=False)\n",
        "\n",
        "df_array_cols = df.select(col(\"name\"), col(\"scores\")[0].alias(\"score1\"),col(\"scores\")[1].alias(\"score2\"),col(\"scores\")[2].alias(\"score3\"))\n",
        "df_array_cols.show()\n",
        "\n",
        "df = df.withColumn(\"score_structs\", array(struct(lit(\"Math\").alias(\"subject\"), lit(85).alias(\"score\")),struct(lit(\"Physics\").alias(\"subject\"), lit(90).alias(\"score\"))))\n",
        "df.show(truncate=False)\n",
        "\n",
        "df_explode = df.withColumn(\"score\", explode(col(\"scores\")))\n",
        "df_explode.show()\n",
        "\n",
        "df_explode_struct = df.withColumn(\"struct_item\", explode(col(\"score_structs\")))\n",
        "df_explode_struct.select(\"name\", \"struct_item.subject\", \"struct_item.score\").show()\n",
        "\n",
        "df_map_array = df.withColumn(\"map_array\", array(col(\"subject_scores\")))\n",
        "df_map_explode = df_map_array.withColumn(\"map_item\", explode(col(\"map_array\")))\n",
        "df_map_explode.show(truncate=False)\n",
        "\n",
        "data_nested = [(\"Alice\", [[1,2],[3,4]]), (\"Bob\", [[5,6],[7,8]])]\n",
        "df_nested = spark.createDataFrame(data_nested, [\"name\",\"nested_array\"])\n",
        "df_nested.show(truncate=False)\n",
        "\n",
        "df_explode_nested = df_nested.withColumn(\"exploded_array\", explode(col(\"nested_array\")))\n",
        "df_explode_nested.show(truncate=False)\n",
        "\n",
        "df_flattened = df_nested.withColumn(\"flattened_array\", flatten(col(\"nested_array\")))\n",
        "df_flattened.show(truncate=False)\n",
        "\n",
        "df_array_string = spark.createDataFrame([(\"Alice\", [\"A\",\"B\",\"C\"])], [\"name\",\"letters\"])\n",
        "df_array_string = df_array_string.withColumn(\"letters_str\", concat_ws(\",\", col(\"letters\")))\n",
        "df_array_string.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSz-SiHh8vb6",
        "outputId": "4bdaeab4-c41e-4b25-b963-9a4d0cdac112"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------------+\n",
            "|name |age|scores      |\n",
            "+-----+---+------------+\n",
            "|Alice|25 |[85, 90, 95]|\n",
            "|Bob  |30 |[85, 90, 95]|\n",
            "+-----+---+------------+\n",
            "\n",
            "+-----+---+------------+---------------------------+\n",
            "|name |age|scores      |subject_scores             |\n",
            "+-----+---+------------+---------------------------+\n",
            "|Alice|25 |[85, 90, 95]|{Math -> 85, Physics -> 90}|\n",
            "|Bob  |30 |[85, 90, 95]|{Math -> 85, Physics -> 90}|\n",
            "+-----+---+------------+---------------------------+\n",
            "\n",
            "+-----+------+------+------+\n",
            "| name|score1|score2|score3|\n",
            "+-----+------+------+------+\n",
            "|Alice|    85|    90|    95|\n",
            "|  Bob|    85|    90|    95|\n",
            "+-----+------+------+------+\n",
            "\n",
            "+-----+---+------------+---------------------------+---------------------------+\n",
            "|name |age|scores      |subject_scores             |score_structs              |\n",
            "+-----+---+------------+---------------------------+---------------------------+\n",
            "|Alice|25 |[85, 90, 95]|{Math -> 85, Physics -> 90}|[{Math, 85}, {Physics, 90}]|\n",
            "|Bob  |30 |[85, 90, 95]|{Math -> 85, Physics -> 90}|[{Math, 85}, {Physics, 90}]|\n",
            "+-----+---+------------+---------------------------+---------------------------+\n",
            "\n",
            "+-----+---+------------+--------------------+--------------------+-----+\n",
            "| name|age|      scores|      subject_scores|       score_structs|score|\n",
            "+-----+---+------------+--------------------+--------------------+-----+\n",
            "|Alice| 25|[85, 90, 95]|{Math -> 85, Phys...|[{Math, 85}, {Phy...|   85|\n",
            "|Alice| 25|[85, 90, 95]|{Math -> 85, Phys...|[{Math, 85}, {Phy...|   90|\n",
            "|Alice| 25|[85, 90, 95]|{Math -> 85, Phys...|[{Math, 85}, {Phy...|   95|\n",
            "|  Bob| 30|[85, 90, 95]|{Math -> 85, Phys...|[{Math, 85}, {Phy...|   85|\n",
            "|  Bob| 30|[85, 90, 95]|{Math -> 85, Phys...|[{Math, 85}, {Phy...|   90|\n",
            "|  Bob| 30|[85, 90, 95]|{Math -> 85, Phys...|[{Math, 85}, {Phy...|   95|\n",
            "+-----+---+------------+--------------------+--------------------+-----+\n",
            "\n",
            "+-----+-------+-----+\n",
            "| name|subject|score|\n",
            "+-----+-------+-----+\n",
            "|Alice|   Math|   85|\n",
            "|Alice|Physics|   90|\n",
            "|  Bob|   Math|   85|\n",
            "|  Bob|Physics|   90|\n",
            "+-----+-------+-----+\n",
            "\n",
            "+-----+---+------------+---------------------------+---------------------------+-----------------------------+---------------------------+\n",
            "|name |age|scores      |subject_scores             |score_structs              |map_array                    |map_item                   |\n",
            "+-----+---+------------+---------------------------+---------------------------+-----------------------------+---------------------------+\n",
            "|Alice|25 |[85, 90, 95]|{Math -> 85, Physics -> 90}|[{Math, 85}, {Physics, 90}]|[{Math -> 85, Physics -> 90}]|{Math -> 85, Physics -> 90}|\n",
            "|Bob  |30 |[85, 90, 95]|{Math -> 85, Physics -> 90}|[{Math, 85}, {Physics, 90}]|[{Math -> 85, Physics -> 90}]|{Math -> 85, Physics -> 90}|\n",
            "+-----+---+------------+---------------------------+---------------------------+-----------------------------+---------------------------+\n",
            "\n",
            "+-----+----------------+\n",
            "|name |nested_array    |\n",
            "+-----+----------------+\n",
            "|Alice|[[1, 2], [3, 4]]|\n",
            "|Bob  |[[5, 6], [7, 8]]|\n",
            "+-----+----------------+\n",
            "\n",
            "+-----+----------------+--------------+\n",
            "|name |nested_array    |exploded_array|\n",
            "+-----+----------------+--------------+\n",
            "|Alice|[[1, 2], [3, 4]]|[1, 2]        |\n",
            "|Alice|[[1, 2], [3, 4]]|[3, 4]        |\n",
            "|Bob  |[[5, 6], [7, 8]]|[5, 6]        |\n",
            "|Bob  |[[5, 6], [7, 8]]|[7, 8]        |\n",
            "+-----+----------------+--------------+\n",
            "\n",
            "+-----+----------------+---------------+\n",
            "|name |nested_array    |flattened_array|\n",
            "+-----+----------------+---------------+\n",
            "|Alice|[[1, 2], [3, 4]]|[1, 2, 3, 4]   |\n",
            "|Bob  |[[5, 6], [7, 8]]|[5, 6, 7, 8]   |\n",
            "+-----+----------------+---------------+\n",
            "\n",
            "+-----+---------+-----------+\n",
            "| name|  letters|letters_str|\n",
            "+-----+---------+-----------+\n",
            "|Alice|[A, B, C]|      A,B,C|\n",
            "+-----+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Practical4_Aggregates\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (\"Alice\", \"HR\", 3000),\n",
        "    (\"Bob\", \"HR\", 4000),\n",
        "    (\"Cathy\", \"IT\", 5000),\n",
        "    (\"David\", \"IT\", 6000),\n",
        "    (\"Eva\", \"Finance\", 7000),\n",
        "    (\"Frank\", \"Finance\", 8000),\n",
        "    (\"George\", \"HR\", 3000)\n",
        "]\n",
        "schema = [\"name\", \"dept\", \"salary\"]\n",
        "df = spark.createDataFrame(data, schema)\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# 1. Group rows in DataFrame (sum, avg, max, min)\n",
        "grouped_df = df.groupBy(\"dept\").agg(\n",
        "    count(\"*\").alias(\"employee_count\"),\n",
        "    avg(\"salary\").alias(\"avg_salary\"),\n",
        "    sum(\"salary\").alias(\"total_salary\"),\n",
        "    max(\"salary\").alias(\"max_salary\"),\n",
        "    min(\"salary\").alias(\"min_salary\")\n",
        ")\n",
        "print(\"Grouped DataFrame with aggregates:\")\n",
        "grouped_df.show()\n",
        "\n",
        "# 2. Get Count distinct on DataFrame\n",
        "distinct_count = df.select(countDistinct(\"dept\").alias(\"distinct_departments\"))\n",
        "print(\"Distinct count of departments:\")\n",
        "distinct_count.show()\n",
        "\n",
        "# 3. Add row number to DataFrame (Window function)\n",
        "windowSpec = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n",
        "df_with_rownum = df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
        "print(\"DataFrame with row numbers:\")\n",
        "df_with_rownum.show()\n",
        "\n",
        "# 4. Select the first row of each group (highest salary in each dept)\n",
        "first_row_df = df_with_rownum.filter(col(\"row_number\") == 1).drop(\"row_number\")\n",
        "print(\"First row of each group (Top salary per dept):\")\n",
        "first_row_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FirUEcbH9AFj",
        "outputId": "8e927794-2db9-4b14-97ad-8d7fdc1567a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+------+-------+------+\n",
            "|  name|   dept|salary|\n",
            "+------+-------+------+\n",
            "| Alice|     HR|  3000|\n",
            "|   Bob|     HR|  4000|\n",
            "| Cathy|     IT|  5000|\n",
            "| David|     IT|  6000|\n",
            "|   Eva|Finance|  7000|\n",
            "| Frank|Finance|  8000|\n",
            "|George|     HR|  3000|\n",
            "+------+-------+------+\n",
            "\n",
            "Grouped DataFrame with aggregates:\n",
            "+-------+--------------+------------------+------------+----------+----------+\n",
            "|   dept|employee_count|        avg_salary|total_salary|max_salary|min_salary|\n",
            "+-------+--------------+------------------+------------+----------+----------+\n",
            "|     HR|             3|3333.3333333333335|       10000|      4000|      3000|\n",
            "|     IT|             2|            5500.0|       11000|      6000|      5000|\n",
            "|Finance|             2|            7500.0|       15000|      8000|      7000|\n",
            "+-------+--------------+------------------+------------+----------+----------+\n",
            "\n",
            "Distinct count of departments:\n",
            "+--------------------+\n",
            "|distinct_departments|\n",
            "+--------------------+\n",
            "|                   3|\n",
            "+--------------------+\n",
            "\n",
            "DataFrame with row numbers:\n",
            "+------+-------+------+----------+\n",
            "|  name|   dept|salary|row_number|\n",
            "+------+-------+------+----------+\n",
            "| Frank|Finance|  8000|         1|\n",
            "|   Eva|Finance|  7000|         2|\n",
            "|   Bob|     HR|  4000|         1|\n",
            "| Alice|     HR|  3000|         2|\n",
            "|George|     HR|  3000|         3|\n",
            "| David|     IT|  6000|         1|\n",
            "| Cathy|     IT|  5000|         2|\n",
            "+------+-------+------+----------+\n",
            "\n",
            "First row of each group (Top salary per dept):\n",
            "+-----+-------+------+\n",
            "| name|   dept|salary|\n",
            "+-----+-------+------+\n",
            "|Frank|Finance|  8000|\n",
            "|  Bob|     HR|  4000|\n",
            "|David|     IT|  6000|\n",
            "+-----+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Practical5_Joins_SQL\").getOrCreate()\n",
        "\n",
        "# Sample DataFrames\n",
        "emp_data = [\n",
        "    (1, \"Alice\", 101, 3000),\n",
        "    (2, \"Bob\", 102, 4000),\n",
        "    (3, \"Cathy\", 101, 5000),\n",
        "    (4, \"David\", 103, 6000),\n",
        "    (5, \"Eva\", 104, 7000)\n",
        "]\n",
        "dept_data = [\n",
        "    (101, \"HR\"),\n",
        "    (102, \"IT\"),\n",
        "    (103, \"Finance\")\n",
        "]\n",
        "\n",
        "emp_schema = [\"emp_id\", \"name\", \"dept_id\", \"salary\"]\n",
        "dept_schema = [\"dept_id\", \"dept_name\"]\n",
        "\n",
        "emp_df = spark.createDataFrame(emp_data, emp_schema)\n",
        "dept_df = spark.createDataFrame(dept_data, dept_schema)\n",
        "\n",
        "print(\"Employees:\")\n",
        "emp_df.show()\n",
        "print(\"Departments:\")\n",
        "dept_df.show()\n",
        "\n",
        "# 1. Spark SQL Join (register as temp view and run SQL)\n",
        "emp_df.createOrReplaceTempView(\"employees\")\n",
        "dept_df.createOrReplaceTempView(\"departments\")\n",
        "\n",
        "sql_join = spark.sql(\"\"\"\n",
        "SELECT e.emp_id, e.name, d.dept_name, e.salary\n",
        "FROM employees e\n",
        "JOIN departments d ON e.dept_id = d.dept_id\n",
        "\"\"\")\n",
        "print(\"SQL Join:\")\n",
        "sql_join.show()\n",
        "\n",
        "# 2. Join multiple DataFrames\n",
        "extra_data = [(101, \"Mumbai\"), (102, \"Delhi\"), (103, \"Pune\"), (104, \"Chennai\")]\n",
        "loc_df = spark.createDataFrame(extra_data, [\"dept_id\", \"location\"])\n",
        "\n",
        "multi_join = emp_df.join(dept_df, \"dept_id\").join(loc_df, \"dept_id\")\n",
        "print(\"Join multiple DataFrames:\")\n",
        "multi_join.show()\n",
        "\n",
        "# 3. Inner join two tables/DataFrames\n",
        "inner_join = emp_df.join(dept_df, emp_df.dept_id == dept_df.dept_id, \"inner\")\n",
        "print(\"Inner Join:\")\n",
        "inner_join.show()\n",
        "\n",
        "# 4. Self Join\n",
        "self_join = emp_df.alias(\"a\").join(emp_df.alias(\"b\"), col(\"a.dept_id\") == col(\"b.dept_id\"))\n",
        "print(\"Self Join (employees in same dept):\")\n",
        "self_join.select(\"a.name\", \"b.name\", \"a.dept_id\").show()\n",
        "\n",
        "# 5. Join tables on multiple columns\n",
        "multi_col_join = emp_df.join(dept_df, (emp_df.dept_id == dept_df.dept_id) & (emp_df.salary > 4000))\n",
        "print(\"Join on multiple columns:\")\n",
        "multi_col_join.show()\n",
        "\n",
        "# 6. Convert case class to a schema (in PySpark we use StructType)\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)\n",
        "])\n",
        "df_schema = spark.createDataFrame([(1, \"Alex\", 30), (2, \"Bella\", 25)], schema)\n",
        "print(\"DataFrame with StructType Schema:\")\n",
        "df_schema.show()\n",
        "\n",
        "# 7. Create array of struct column\n",
        "array_struct_df = emp_df.withColumn(\"array_struct\", array(struct(\"emp_id\", \"name\")))\n",
        "print(\"Array of struct column:\")\n",
        "array_struct_df.show(truncate=False)\n",
        "\n",
        "# 8. Flatten nested column\n",
        "nested_data = [(1, (\"Alice\", (\"HR\", 3000)))]\n",
        "nested_schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"emp\", StructType([\n",
        "        StructField(\"name\", StringType()),\n",
        "        StructField(\"details\", StructType([\n",
        "            StructField(\"dept\", StringType()),\n",
        "            StructField(\"salary\", IntegerType())\n",
        "        ]))\n",
        "    ]))\n",
        "])\n",
        "nested_df = spark.createDataFrame(nested_data, nested_schema)\n",
        "flat_df = nested_df.select(\"id\", col(\"emp.name\").alias(\"emp_name\"),\n",
        "                           col(\"emp.details.dept\").alias(\"department\"),\n",
        "                           col(\"emp.details.salary\").alias(\"salary\"))\n",
        "print(\"Flatten nested column:\")\n",
        "flat_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7oA1hud9O53",
        "outputId": "59aff98b-149e-4c0b-f432-2518202a5d37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees:\n",
            "+------+-----+-------+------+\n",
            "|emp_id| name|dept_id|salary|\n",
            "+------+-----+-------+------+\n",
            "|     1|Alice|    101|  3000|\n",
            "|     2|  Bob|    102|  4000|\n",
            "|     3|Cathy|    101|  5000|\n",
            "|     4|David|    103|  6000|\n",
            "|     5|  Eva|    104|  7000|\n",
            "+------+-----+-------+------+\n",
            "\n",
            "Departments:\n",
            "+-------+---------+\n",
            "|dept_id|dept_name|\n",
            "+-------+---------+\n",
            "|    101|       HR|\n",
            "|    102|       IT|\n",
            "|    103|  Finance|\n",
            "+-------+---------+\n",
            "\n",
            "SQL Join:\n",
            "+------+-----+---------+------+\n",
            "|emp_id| name|dept_name|salary|\n",
            "+------+-----+---------+------+\n",
            "|     3|Cathy|       HR|  5000|\n",
            "|     1|Alice|       HR|  3000|\n",
            "|     2|  Bob|       IT|  4000|\n",
            "|     4|David|  Finance|  6000|\n",
            "+------+-----+---------+------+\n",
            "\n",
            "Join multiple DataFrames:\n",
            "+-------+------+-----+------+---------+--------+\n",
            "|dept_id|emp_id| name|salary|dept_name|location|\n",
            "+-------+------+-----+------+---------+--------+\n",
            "|    101|     3|Cathy|  5000|       HR|  Mumbai|\n",
            "|    101|     1|Alice|  3000|       HR|  Mumbai|\n",
            "|    102|     2|  Bob|  4000|       IT|   Delhi|\n",
            "|    103|     4|David|  6000|  Finance|    Pune|\n",
            "+-------+------+-----+------+---------+--------+\n",
            "\n",
            "Inner Join:\n",
            "+------+-----+-------+------+-------+---------+\n",
            "|emp_id| name|dept_id|salary|dept_id|dept_name|\n",
            "+------+-----+-------+------+-------+---------+\n",
            "|     3|Cathy|    101|  5000|    101|       HR|\n",
            "|     1|Alice|    101|  3000|    101|       HR|\n",
            "|     2|  Bob|    102|  4000|    102|       IT|\n",
            "|     4|David|    103|  6000|    103|  Finance|\n",
            "+------+-----+-------+------+-------+---------+\n",
            "\n",
            "Self Join (employees in same dept):\n",
            "+-----+-----+-------+\n",
            "| name| name|dept_id|\n",
            "+-----+-----+-------+\n",
            "|Alice|Cathy|    101|\n",
            "|Alice|Alice|    101|\n",
            "|  Bob|  Bob|    102|\n",
            "|  Eva|  Eva|    104|\n",
            "|Cathy|Cathy|    101|\n",
            "|Cathy|Alice|    101|\n",
            "|David|David|    103|\n",
            "+-----+-----+-------+\n",
            "\n",
            "Join on multiple columns:\n",
            "+------+-----+-------+------+-------+---------+\n",
            "|emp_id| name|dept_id|salary|dept_id|dept_name|\n",
            "+------+-----+-------+------+-------+---------+\n",
            "|     3|Cathy|    101|  5000|    101|       HR|\n",
            "|     4|David|    103|  6000|    103|  Finance|\n",
            "+------+-----+-------+------+-------+---------+\n",
            "\n",
            "DataFrame with StructType Schema:\n",
            "+---+-----+---+\n",
            "| id| name|age|\n",
            "+---+-----+---+\n",
            "|  1| Alex| 30|\n",
            "|  2|Bella| 25|\n",
            "+---+-----+---+\n",
            "\n",
            "Array of struct column:\n",
            "+------+-----+-------+------+------------+\n",
            "|emp_id|name |dept_id|salary|array_struct|\n",
            "+------+-----+-------+------+------------+\n",
            "|1     |Alice|101    |3000  |[{1, Alice}]|\n",
            "|2     |Bob  |102    |4000  |[{2, Bob}]  |\n",
            "|3     |Cathy|101    |5000  |[{3, Cathy}]|\n",
            "|4     |David|103    |6000  |[{4, David}]|\n",
            "|5     |Eva  |104    |7000  |[{5, Eva}]  |\n",
            "+------+-----+-------+------+------------+\n",
            "\n",
            "Flatten nested column:\n",
            "+---+--------+----------+------+\n",
            "| id|emp_name|department|salary|\n",
            "+---+--------+----------+------+\n",
            "|  1|   Alice|        HR|  3000|\n",
            "+---+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, when, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkSQL_Practical8\").getOrCreate()\n",
        "data = [(1,\"Alice\",23), (2,\"Bob\",30), (3,\"Charlie\",25), (4,\"David\",30)]\n",
        "df = spark.createDataFrame(data, [\"id\",\"name\",\"age\"])\n",
        "df.show()\n",
        "df.filter(df.age > 25).show()\n",
        "df.where(col(\"name\") == \"Alice\").show()\n",
        "\n",
        "df2 = df.withColumn(\"age_plus_5\", col(\"age\") + 5)\n",
        "df2.show()\n",
        "df3 = df.withColumnRenamed(\"name\",\"full_name\")\n",
        "df3.show()\n",
        "df4 = df.drop(\"age\")\n",
        "df4.show()\n",
        "df.distinct().show()\n",
        "df.groupBy(\"age\").count().show()\n",
        "dept = [(1,\"HR\"), (2,\"IT\"), (3,\"Finance\")]\n",
        "df_dept = spark.createDataFrame(dept, [\"id\",\"dept\"])\n",
        "\n",
        "df_join = df.join(df_dept, on=\"id\", how=\"inner\")\n",
        "df_join.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Edz9n6h9Yoj",
        "outputId": "2f694040-7942-44ab-a576-bf5fd4d1a343"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 23|\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 25|\n",
            "|  4|  David| 30|\n",
            "+---+-------+---+\n",
            "\n",
            "+---+-----+---+\n",
            "| id| name|age|\n",
            "+---+-----+---+\n",
            "|  2|  Bob| 30|\n",
            "|  4|David| 30|\n",
            "+---+-----+---+\n",
            "\n",
            "+---+-----+---+\n",
            "| id| name|age|\n",
            "+---+-----+---+\n",
            "|  1|Alice| 23|\n",
            "+---+-----+---+\n",
            "\n",
            "+---+-------+---+----------+\n",
            "| id|   name|age|age_plus_5|\n",
            "+---+-------+---+----------+\n",
            "|  1|  Alice| 23|        28|\n",
            "|  2|    Bob| 30|        35|\n",
            "|  3|Charlie| 25|        30|\n",
            "|  4|  David| 30|        35|\n",
            "+---+-------+---+----------+\n",
            "\n",
            "+---+---------+---+\n",
            "| id|full_name|age|\n",
            "+---+---------+---+\n",
            "|  1|    Alice| 23|\n",
            "|  2|      Bob| 30|\n",
            "|  3|  Charlie| 25|\n",
            "|  4|    David| 30|\n",
            "+---+---------+---+\n",
            "\n",
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|  Alice|\n",
            "|  2|    Bob|\n",
            "|  3|Charlie|\n",
            "|  4|  David|\n",
            "+---+-------+\n",
            "\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 23|\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 25|\n",
            "|  4|  David| 30|\n",
            "+---+-------+---+\n",
            "\n",
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 23|    1|\n",
            "| 30|    2|\n",
            "| 25|    1|\n",
            "+---+-----+\n",
            "\n",
            "+---+-------+---+-------+\n",
            "| id|   name|age|   dept|\n",
            "+---+-------+---+-------+\n",
            "|  1|  Alice| 23|     HR|\n",
            "|  2|    Bob| 30|     IT|\n",
            "|  3|Charlie| 25|Finance|\n",
            "+---+-------+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = df.rdd\n",
        "\n",
        "mapped = rdd.map(lambda x: (x[1], x[2]+1)).collect()\n",
        "print(\"map:\", mapped)\n",
        "\n",
        "mapped_part = rdd.mapPartitions(lambda part: [(x[1], x[2]*2) for x in part]).collect()\n",
        "print(\"mapPartitions:\", mapped_part)\n",
        "print(\"foreach output:\")\n",
        "df.rdd.foreach(lambda x: print(x))\n",
        "\n",
        "print(\"foreachPartition output:\")\n",
        "df.rdd.foreachPartition(lambda part: [print(\"Partition:\", list(part))])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf9QYuZa9e7c",
        "outputId": "84758f7c-1e35-4eee-bea3-30c7731ab0f9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "map: [('Alice', 24), ('Bob', 31), ('Charlie', 26), ('David', 31)]\n",
            "mapPartitions: [('Alice', 46), ('Bob', 60), ('Charlie', 50), ('David', 60)]\n",
            "foreach output:\n",
            "foreachPartition output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sales = [(\"Alice\",\"Jan\",200),\n",
        "         (\"Alice\",\"Feb\",250),\n",
        "         (\"Bob\",\"Jan\",300),\n",
        "         (\"Bob\",\"Feb\",100)]\n",
        "\n",
        "df_sales = spark.createDataFrame(sales, [\"name\",\"month\",\"amount\"])\n",
        "pivoted = df_sales.groupBy(\"name\").pivot(\"month\").sum(\"amount\")\n",
        "pivoted.show()\n",
        "df_a = spark.createDataFrame([(5,\"Eve\",28)], [\"id\",\"name\",\"age\"])\n",
        "df_union = df.union(df_a)\n",
        "df_union.show()\n",
        "collected = df.collect()\n",
        "print(\"collect():\", collected)\n",
        "df_cached = df.cache()\n",
        "print(\"Cached count:\", df_cached.count())\n",
        "\n",
        "from pyspark import StorageLevel\n",
        "df_persisted = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "print(\"Persisted count:\", df_persisted.count())\n",
        "def greet(name):\n",
        "    return \"Hello \" + name\n",
        "\n",
        "greet_udf = udf(greet, StringType())\n",
        "df_udf = df.withColumn(\"greeting\", greet_udf(col(\"name\")))\n",
        "df_udf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qS6HSPg9iI7",
        "outputId": "0abc9d2e-2306-4775-f428-2c36ff77f97d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+---+\n",
            "| name|Feb|Jan|\n",
            "+-----+---+---+\n",
            "|Alice|250|200|\n",
            "|  Bob|100|300|\n",
            "+-----+---+---+\n",
            "\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 23|\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 25|\n",
            "|  4|  David| 30|\n",
            "|  5|    Eve| 28|\n",
            "+---+-------+---+\n",
            "\n",
            "collect(): [Row(id=1, name='Alice', age=23), Row(id=2, name='Bob', age=30), Row(id=3, name='Charlie', age=25), Row(id=4, name='David', age=30)]\n",
            "Cached count: 4\n",
            "Persisted count: 4\n",
            "+---+-------+---+-------------+\n",
            "| id|   name|age|     greeting|\n",
            "+---+-------+---+-------------+\n",
            "|  1|  Alice| 23|  Hello Alice|\n",
            "|  2|    Bob| 30|    Hello Bob|\n",
            "|  3|Charlie| 25|Hello Charlie|\n",
            "|  4|  David| 30|  Hello David|\n",
            "+---+-------+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# Spark MLlib Practical 9: Estimator, Transformer, Param\n",
        "# ---------------------------------------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Step 1: Start Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MLlib_Estimator_Transformer_Param\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Create Sample Data\n",
        "data = [\n",
        "    (0.0, 1.0, 0.1, -1.0),\n",
        "    (1.0, 2.0, 1.1, 1.0),\n",
        "    (0.0, 2.0, 1.3, -0.5),\n",
        "    (1.0, 3.0, 1.2, 1.3),\n",
        "    (0.0, 3.0, 0.8, -0.7),\n",
        "]\n",
        "columns = [\"label\", \"feature1\", \"feature2\", \"feature3\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Step 3: Transformer Example (VectorAssembler)\n",
        "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"],\n",
        "                            outputCol=\"features\")\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Step 4: Transformer Example (StandardScaler)\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
        "scaler_model = scaler.fit(df_transformed)   # Estimator produces Transformer\n",
        "df_scaled = scaler_model.transform(df_transformed)\n",
        "\n",
        "# Step 5: Estimator Example (Logistic Regression)\n",
        "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\", maxIter=10)\n",
        "\n",
        "# Step 6: Pipeline (combining everything)\n",
        "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
        "\n",
        "# Fit the pipeline (Estimator -> Model)\n",
        "model = pipeline.fit(df)\n",
        "\n",
        "# Step 7: Make Predictions\n",
        "predictions = model.transform(df)\n",
        "predictions.select(\"label\", \"features\", \"scaledFeatures\", \"prediction\", \"probability\").show()\n",
        "\n",
        "# Step 8: Working with Params\n",
        "print(\"Logistic Regression Parameters:\")\n",
        "print(lr.explainParams())\n",
        "\n",
        "print(\"\\nCurrent Max Iterations Param:\", lr.getOrDefault(\"maxIter\"))\n",
        "print(\"Changing maxIter to 20...\")\n",
        "lr.setMaxIter(20)\n",
        "print(\"Updated Max Iterations Param:\", lr.getOrDefault(\"maxIter\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkaDCD3Q-C_G",
        "outputId": "aed2674e-2668-46c3-95ca-be4d599dca8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------+--------------------+----------+--------------------+\n",
            "|label|      features|      scaledFeatures|prediction|         probability|\n",
            "+-----+--------------+--------------------+----------+--------------------+\n",
            "|  0.0|[1.0,0.1,-1.0]|[1.19522860933439...|       0.0|[0.99988509178520...|\n",
            "|  1.0| [2.0,1.1,1.0]|[2.39045721866878...|       1.0|[0.00101397015652...|\n",
            "|  0.0|[2.0,1.3,-0.5]|[2.39045721866878...|       0.0|[0.99898038439963...|\n",
            "|  1.0| [3.0,1.2,1.3]|[3.58568582800318...|       1.0|[1.07539818686704...|\n",
            "|  0.0|[3.0,0.8,-0.7]|[3.58568582800318...|       0.0|[0.99970044220819...|\n",
            "+-----+--------------+--------------------+----------+--------------------+\n",
            "\n",
            "Logistic Regression Parameters:\n",
            "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
            "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
            "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
            "featuresCol: features column name. (default: features, current: scaledFeatures)\n",
            "fitIntercept: whether to fit an intercept term. (default: True)\n",
            "labelCol: label column name. (default: label, current: label)\n",
            "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
            "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
            "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
            "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
            "regParam: regularization parameter (>= 0). (default: 0.0)\n",
            "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
            "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
            "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
            "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
            "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
            "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
            "\n",
            "Current Max Iterations Param: 10\n",
            "Changing maxIter to 20...\n",
            "Updated Max Iterations Param: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JdGF4-vL9srp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7LXMwXuCBLzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PYQs Practice"
      ],
      "metadata": {
        "id": "9jtgM3XvBNaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 a) Create 2 text files. Read the contents in a single RDD. b) Create 2 CSV files. Read the contents in a single RDD. 2) Create two dataframes one for employee and other for dept. Perform a)\n",
        "# Left outer join\n",
        "# b) Full outer join\n",
        "# c) Inner join\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Q1\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# Step 2: Create the necessary files for the question\n",
        "with open(\"file1.txt\", \"w\") as f:\n",
        "  f.write(\"this is the first text file\\n\")\n",
        "  f.write(\"it has two lines\\n\")\n",
        "with open(\"file2.txt\", \"w\") as f:\n",
        "  f.write(\"this is the second text file\\n\")\n",
        "with open(\"data1.csv\", \"w\") as f:\n",
        "  f.write(\"id,value\\n\")\n",
        "  f.write(\"1,a\\n\")\n",
        "  f.write(\"2,b\\n\")\n",
        "with open(\"data2.csv\", \"w\") as f:\n",
        "  f.write(\"id,value\\n\")\n",
        "  f.write(\"3,c\\n\")\n",
        "  f.write(\"4,d\\n\")\n",
        "  print(\"--- Sample files for Question 1 created successfully ---\\n\")\n",
        "# --- SOLUTION --- # a) Read 2 text files into a single RDD\n",
        "print(\"\\nReading contents from two text files into a single RDD:\")\n",
        "text_rdd = sc.textFile(\"file1.txt,file2.txt\")\n",
        "print(\"Result:\", text_rdd.collect())\n",
        "# a) Read 2 CSV files into a single RDD\n",
        "print(\"\\nReading contents from two CSV files into a single RDD:\")\n",
        "csv_df = spark.read.csv([\"data1.csv\", \"data2.csv\"], header=True)\n",
        "csv_rdd = csv_df.rdd\n",
        "print(\"Result:\", csv_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQsewwXxBLnU",
        "outputId": "fe704ead-baa8-43b2-db4c-65ebf305f030"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "--- Sample files for Question 1 created successfully ---\n",
            "\n",
            "\n",
            "Reading contents from two text files into a single RDD:\n",
            "Result: ['this is the first text file', 'it has two lines', 'this is the second text file']\n",
            "\n",
            "Reading contents from two CSV files into a single RDD:\n",
            "Result: [Row(id='1', value='a'), Row(id='2', value='b'), Row(id='3', value='c'), Row(id='4', value='d')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the SparkSession 'spark' is already created from the previous question. # --- SETUP --- # Create the employee and department DataFrames\n",
        "emp_data = [(1, \"Smith\", 10), (2, \"Rose\", 20), (3, \"Williams\", 10), (4, \"Jones\", 30)]\n",
        "dept_data = [(\"Finance\", 10), (\"Marketing\", 20), (\"Sales\", 30), (\"IT\", 40)]\n",
        "emp_df = spark.createDataFrame(emp_data, [\"emp_id\", \"name\", \"dept_id\"])\n",
        "dept_df = spark.createDataFrame(dept_data, [\"dept_name\", \"dept_id\"])\n",
        "print(\"Employee DataFrame:\")\n",
        "emp_df.show()\n",
        "print(\"Department DataFrame:\")\n",
        "dept_df.show()\n",
        "# --- SOLUTION --- # a) Perform Left outer join\n",
        "print(\"\\na) Left Outer Join Result:\")\n",
        "emp_df.join(dept_df, on=\"dept_id\", how=\"left_outer\").show()\n",
        "# b) Perform Full outer join\n",
        "print(\"\\nb) Full Outer Join Result:\")\n",
        "emp_df.join(dept_df, on=\"dept_id\", how=\"full_outer\").show()\n",
        "# c) Perform Inner join\n",
        "print(\"\\nc) Inner Join Result:\")\n",
        "emp_df.join(dept_df, on=\"dept_id\", how=\"inner\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJtCpjWQBlmA",
        "outputId": "2cb61d1d-da30-4481-b153-9c1ab66436bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee DataFrame:\n",
            "+------+--------+-------+\n",
            "|emp_id|    name|dept_id|\n",
            "+------+--------+-------+\n",
            "|     1|   Smith|     10|\n",
            "|     2|    Rose|     20|\n",
            "|     3|Williams|     10|\n",
            "|     4|   Jones|     30|\n",
            "+------+--------+-------+\n",
            "\n",
            "Department DataFrame:\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|  Finance|     10|\n",
            "|Marketing|     20|\n",
            "|    Sales|     30|\n",
            "|       IT|     40|\n",
            "+---------+-------+\n",
            "\n",
            "\n",
            "a) Left Outer Join Result:\n",
            "+-------+------+--------+---------+\n",
            "|dept_id|emp_id|    name|dept_name|\n",
            "+-------+------+--------+---------+\n",
            "|     10|     1|   Smith|  Finance|\n",
            "|     20|     2|    Rose|Marketing|\n",
            "|     30|     4|   Jones|    Sales|\n",
            "|     10|     3|Williams|  Finance|\n",
            "+-------+------+--------+---------+\n",
            "\n",
            "\n",
            "b) Full Outer Join Result:\n",
            "+-------+------+--------+---------+\n",
            "|dept_id|emp_id|    name|dept_name|\n",
            "+-------+------+--------+---------+\n",
            "|     10|     1|   Smith|  Finance|\n",
            "|     10|     3|Williams|  Finance|\n",
            "|     20|     2|    Rose|Marketing|\n",
            "|     30|     4|   Jones|    Sales|\n",
            "|     40|  NULL|    NULL|       IT|\n",
            "+-------+------+--------+---------+\n",
            "\n",
            "\n",
            "c) Inner Join Result:\n",
            "+-------+------+--------+---------+\n",
            "|dept_id|emp_id|    name|dept_name|\n",
            "+-------+------+--------+---------+\n",
            "|     10|     3|Williams|  Finance|\n",
            "|     10|     1|   Smith|  Finance|\n",
            "|     20|     2|    Rose|Marketing|\n",
            "|     30|     4|   Jones|    Sales|\n",
            "+-------+------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) For the following data and schema create a dataframe and perform the given\n",
        "# operations\n",
        "# Data: Seq(Row(Row(\"James;\",\"\",\"Smith\"),\"36636\",\"M\",\"20000\"), Row(Row(\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"40000\"), Row(Row(\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"10000\"), Row(Row(\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"45000\"), Row(Row(\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\")\n",
        "# )\n",
        "# Schema should have the columns as: firstname, middlename, lastname, dob, gender, expenses All\n",
        "# columns will be of type String\n",
        "# Perform the following operations:\n",
        "# a) Change the data type of expenses to Integer\n",
        "# b) Rename dob to DateOfBirth\n",
        "# c) Create a column that has value expense*5\n",
        "# 202 Create a data frame with a nested array column. Perform the following\n",
        "# operations:\n",
        "# a) Flatten nested array\n",
        "# b) Explode nested array\n",
        "# c) Convert array of string to string column.\n",
        "\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip2_Q1\").getOrCreate()\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SETUP --- # Define the data and schema\n",
        "data = [\n",
        "Row(name=Row(firstname=\"James\", middlename=\"\", lastname=\"Smith\"), dob=\"36636\", gender=\"M\", expenses=\"20000\"), Row(name=Row(firstname=\"Michael\", middlename=\"Rose\", lastname=\"\"), dob=\"40288\", gender=\"M\", expenses=\"40000\"), Row(name=Row(firstname=\"Robert\", middlename=\"\", lastname=\"Williams\"), dob=\"42114\", gender=\"M\", expenses=\"10000\"), Row(name=Row(firstname=\"Maria\", middlename=\"Anne\", lastname=\"Jones\"), dob=\"39192\", gender=\"F\", expenses=\"45000\"), Row(name=Row(firstname=\"Jen\", middlename=\"Mary\", lastname=\"Brown\"), dob=\"\", gender=\"F\", expenses=\"-1\")\n",
        "]\n",
        "schema = StructType([\n",
        "StructField(\"name\", StructType([\n",
        "StructField(\"firstname\", StringType(), True), StructField(\"middlename\", StringType(), True), StructField(\"lastname\", StringType(), True)\n",
        "])), StructField(\"dob\", StringType(), True), StructField(\"gender\", StringType(), True), StructField(\"expenses\", StringType(), True)\n",
        "])\n",
        "# Create the DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "print(\"Original DataFrame:\")\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "# --- SOLUTION --- # a) Change the data type of expenses to Integer\n",
        "print(\"\\na) Changing 'expenses' to Integer type:\")\n",
        "df_a = df.withColumn(\"expenses\", col(\"expenses\").cast(IntegerType()))\n",
        "df_a.printSchema()\n",
        "df_a.show()\n",
        "# b) Rename dob to DateOfBirth\n",
        "print(\"\\nb) Renaming 'dob' to 'DateOfBirth':\")\n",
        "df_b = df_a.withColumnRenamed(\"dob\", \"DateOfBirth\")\n",
        "df_b.show()\n",
        "# c) Create a column that has value expense*5\n",
        "print(\"\\nc) Creating a 'bonus' column with 'expenses * 5':\")\n",
        "df_c = df_b.withColumn(\"bonus\", col(\"expenses\") * 5)\n",
        "df_c.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpthtPkgCCO8",
        "outputId": "b2c65b81-26b8-468b-cfc7-a934bb57e02a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "Original DataFrame:\n",
            "+--------------------+-----+------+--------+\n",
            "|name                |dob  |gender|expenses|\n",
            "+--------------------+-----+------+--------+\n",
            "|{James, , Smith}    |36636|M     |20000   |\n",
            "|{Michael, Rose, }   |40288|M     |40000   |\n",
            "|{Robert, , Williams}|42114|M     |10000   |\n",
            "|{Maria, Anne, Jones}|39192|F     |45000   |\n",
            "|{Jen, Mary, Brown}  |     |F     |-1      |\n",
            "+--------------------+-----+------+--------+\n",
            "\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- expenses: string (nullable = true)\n",
            "\n",
            "\n",
            "a) Changing 'expenses' to Integer type:\n",
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- expenses: integer (nullable = true)\n",
            "\n",
            "+--------------------+-----+------+--------+\n",
            "|                name|  dob|gender|expenses|\n",
            "+--------------------+-----+------+--------+\n",
            "|    {James, , Smith}|36636|     M|   20000|\n",
            "|   {Michael, Rose, }|40288|     M|   40000|\n",
            "|{Robert, , Williams}|42114|     M|   10000|\n",
            "|{Maria, Anne, Jones}|39192|     F|   45000|\n",
            "|  {Jen, Mary, Brown}|     |     F|      -1|\n",
            "+--------------------+-----+------+--------+\n",
            "\n",
            "\n",
            "b) Renaming 'dob' to 'DateOfBirth':\n",
            "+--------------------+-----------+------+--------+\n",
            "|                name|DateOfBirth|gender|expenses|\n",
            "+--------------------+-----------+------+--------+\n",
            "|    {James, , Smith}|      36636|     M|   20000|\n",
            "|   {Michael, Rose, }|      40288|     M|   40000|\n",
            "|{Robert, , Williams}|      42114|     M|   10000|\n",
            "|{Maria, Anne, Jones}|      39192|     F|   45000|\n",
            "|  {Jen, Mary, Brown}|           |     F|      -1|\n",
            "+--------------------+-----------+------+--------+\n",
            "\n",
            "\n",
            "c) Creating a 'bonus' column with 'expenses * 5':\n",
            "+--------------------+-----------+------+--------+------+\n",
            "|                name|DateOfBirth|gender|expenses| bonus|\n",
            "+--------------------+-----------+------+--------+------+\n",
            "|    {James, , Smith}|      36636|     M|   20000|100000|\n",
            "|   {Michael, Rose, }|      40288|     M|   40000|200000|\n",
            "|{Robert, , Williams}|      42114|     M|   10000| 50000|\n",
            "|{Maria, Anne, Jones}|      39192|     F|   45000|225000|\n",
            "|  {Jen, Mary, Brown}|           |     F|      -1|    -5|\n",
            "+--------------------+-----------+------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. # Assuming the SparkSession 'spark' is already created from the previous question.\n",
        "from pyspark.sql.functions import col, flatten, explode, concat_ws\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
        "# --- SETUP --- # Create the DataFrame with a nested array\n",
        "data = [\n",
        "(\"James\", [[\"Java\", \"Scala\", \"C++\"], [\"Spark\", \"Java\"]]), (\"Michael\", [[\"Spark\", \"Java\", \"C++\"], [\"Spark\", \"Java\"]]), (\"Robert\", [[\"CSharp\", \"VB\"], [\"Spark\", \"Python\"]])\n",
        "]\n",
        "schema = StructType([\n",
        "StructField(\"name\", StringType(), True), StructField(\"subjects\", ArrayType(ArrayType(StringType())), True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema)\n",
        "print(\"Original DataFrame with nested array:\")\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrOog-y0CpRR",
        "outputId": "ea5a255c-6114-4ad8-9df2-bb1b3efa82ac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame with nested array:\n",
            "+-------+-----------------------------------+\n",
            "|name   |subjects                           |\n",
            "+-------+-----------------------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
            "+-------+-----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SOLUTION --- # a) Flatten nested array\n",
        "print(\"\\na) Flattened nested array:\")\n",
        "df_a = df.withColumn(\"subjects_flat\", flatten(col(\"subjects\")))\n",
        "df_a.show(truncate=False)\n",
        "# b) Explode nested array\n",
        "print(\"\\nb) Exploded nested array:\")\n",
        "# Note: Exploding a nested array directly creates rows with the inner arrays. # To explode to individual elements, you must flatten first.\n",
        "df_b = df_a.withColumn(\"subject\", explode(col(\"subjects_flat\")))\n",
        "df_b.show(truncate=False)\n",
        "# c) Convert array of string to string column\n",
        "print(\"\\nc) Converted array to a single string column:\")\n",
        "df_c = df_a.withColumn(\"subjects_string\", concat_ws(\", \", col(\"subjects_flat\")))\n",
        "df_c.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlNedjrOCwA-",
        "outputId": "b67e1b4e-9a8e-4aa9-cef5-b184ec023ce0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "a) Flattened nested array:\n",
            "+-------+-----------------------------------+-------------------------------+\n",
            "|name   |subjects                           |subjects_flat                  |\n",
            "+-------+-----------------------------------+-------------------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |\n",
            "+-------+-----------------------------------+-------------------------------+\n",
            "\n",
            "\n",
            "b) Exploded nested array:\n",
            "+--------------------+-----------+------+--------+\n",
            "|name                |DateOfBirth|gender|expenses|\n",
            "+--------------------+-----------+------+--------+\n",
            "|{James, , Smith}    |36636      |M     |20000   |\n",
            "|{Michael, Rose, }   |40288      |M     |40000   |\n",
            "|{Robert, , Williams}|42114      |M     |10000   |\n",
            "|{Maria, Anne, Jones}|39192      |F     |45000   |\n",
            "|{Jen, Mary, Brown}  |           |F     |-1      |\n",
            "+--------------------+-----------+------+--------+\n",
            "\n",
            "\n",
            "c) Converted array to a single string column:\n",
            "+-------+-----------------------------------+-------------------------------+-----------------------------+\n",
            "|name   |subjects                           |subjects_flat                  |subjects_string              |\n",
            "+-------+-----------------------------------+-------------------------------+-----------------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|Java, Scala, C++, Spark, Java|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|Spark, Java, C++, Spark, Java|\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |CSharp, VB, Spark, Python    |\n",
            "+-------+-----------------------------------+-------------------------------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Create a data frame with today‚Äôs date and timestamp\n",
        "# b) Display the hours, minutes and seconds from the timestamp\n",
        "# 202 For the following employee data showing name, dept and salary, performthe\n",
        "# given operations:\n",
        "# Data: (\"James\", \"Sales\", 3000), (\"Michael\", \"Sales\", 4600), (\"Robert\", \"Sales\", 4100), (\"Maria\", \"Finance\", 3000), (\"James\", \"Sales\", 3000), (\"Scott\", \"Finance\", 3300), (\"Jen\", \"Finance\", 3900), (\"Jeff\", \"Marketing\", 3000), (\"Kumar\", \"Marketing\", 2000), (\"Saif\", \"Sales\", 4100), (Jason\", \"Sales\", 9000), (\"Alice\", \"Finance\", 3700), (\"Jenniffer\", \"Finance\", 8900), (\"Jenson\", \"Marketing\", 9000)\n",
        "# a) Create a data frame for the above data\n",
        "# b) Display average salary\n",
        "# c) Display number of unique departments\n",
        "# d) Display number of employees with unique salary\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import current_date, current_timestamp, hour, minute, second, col\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip3_Q1\").getOrCreate()\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SOLUTION --- # a) Create a data frame with today's date and timestamp\n",
        "# We start with a dummy DataFrame with one row to add columns to.\n",
        "df = spark.range(1)\n",
        "df_with_time = df.withColumn(\"today_date\", current_date()) \\\n",
        ".withColumn(\"current_ts\", current_timestamp())\n",
        "print(\"a) DataFrame with current date and timestamp:\")\n",
        "df_with_time.show(truncate=False)\n",
        "# b) Display the hours, minutes and seconds from the timestamp\n",
        "time_parts_df = df_with_time.withColumn(\"hour\", hour(col(\"current_ts\"))) \\\n",
        ".withColumn(\"minute\", minute(col(\"current_ts\"))) \\\n",
        ".withColumn(\"second\", second(col(\"current_ts\")))\n",
        "print(\"\\nb) Timestamp parts extracted:\")\n",
        "time_parts_df.select(\"current_ts\", \"hour\", \"minute\", \"second\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5upz5HTC1T7",
        "outputId": "77b9d111-1861-4758-d66e-f4b97ab28301"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "a) DataFrame with current date and timestamp:\n",
            "+---+----------+--------------------------+\n",
            "|id |today_date|current_ts                |\n",
            "+---+----------+--------------------------+\n",
            "|0  |2025-10-07|2025-10-07 14:24:31.062619|\n",
            "+---+----------+--------------------------+\n",
            "\n",
            "\n",
            "b) Timestamp parts extracted:\n",
            "+--------------------------+----+------+------+\n",
            "|current_ts                |hour|minute|second|\n",
            "+--------------------------+----+------+------+\n",
            "|2025-10-07 14:24:31.192571|14  |24    |31    |\n",
            "+--------------------------+----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the SparkSession 'spark' is already created from the previous question.\n",
        "from pyspark.sql.functions import avg, countDistinct\n",
        "# --- SETUP --- # The provided data has a typo `(Jason\"`, which has been corrected to `(\"Jason\"`.\n",
        "employee_data = [\n",
        "(\"James\", \"Sales\", 3000), (\"Michael\", \"Sales\", 4600), (\"Robert\", \"Sales\", 4100), (\"Maria\", \"Finance\", 3000), (\"James\", \"Sales\", 3000), (\"Scott\", \"Finance\", 3300), (\"Jen\", \"Finance\", 3900), (\"Jeff\", \"Marketing\", 3000), (\"Kumar\", \"Marketing\", 2000), (\"Saif\", \"Sales\", 4100), (\"Jason\", \"Sales\", 9000), (\"Alice\", \"Finance\", 3700), (\"Jenniffer\", \"Finance\", 8900), (\"Jenson\", \"Marketing\", 9000)\n",
        "]\n",
        "columns = [\"name\", \"department\", \"salary\"]\n",
        "# --- SOLUTION --- # a) Create a data frame for the above data\n",
        "emp_df = spark.createDataFrame(employee_data, columns)\n",
        "print(\"a) Employee DataFrame:\")\n",
        "emp_df.show()\n",
        "# b) Display average salary\n",
        "avg_salary_df = emp_df.select(avg(\"salary\").alias(\"average_salary\"))\n",
        "print(\"\\nb) Average salary:\")\n",
        "avg_salary_df.show()\n",
        "# c) Display number of unique departments\n",
        "unique_dept_df = emp_df.select(countDistinct(\"department\").alias(\"unique_departments\"))\n",
        "print(\"\\nc) Number of unique departments:\")\n",
        "unique_dept_df.show()\n",
        "# d) Display number of employees with unique salary\n",
        "# This is interpreted as the count of distinct salary values.\n",
        "unique_salary_df = emp_df.select(countDistinct(\"salary\").alias(\"unique_salary_count\"))\n",
        "print(\"\\nd) Number of unique salary values:\")\n",
        "unique_salary_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsR6vJGRDF55",
        "outputId": "f14e6d90-8df7-4ea9-a806-53ce84a20114"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a) Employee DataFrame:\n",
            "+---------+----------+------+\n",
            "|     name|department|salary|\n",
            "+---------+----------+------+\n",
            "|    James|     Sales|  3000|\n",
            "|  Michael|     Sales|  4600|\n",
            "|   Robert|     Sales|  4100|\n",
            "|    Maria|   Finance|  3000|\n",
            "|    James|     Sales|  3000|\n",
            "|    Scott|   Finance|  3300|\n",
            "|      Jen|   Finance|  3900|\n",
            "|     Jeff| Marketing|  3000|\n",
            "|    Kumar| Marketing|  2000|\n",
            "|     Saif|     Sales|  4100|\n",
            "|    Jason|     Sales|  9000|\n",
            "|    Alice|   Finance|  3700|\n",
            "|Jenniffer|   Finance|  8900|\n",
            "|   Jenson| Marketing|  9000|\n",
            "+---------+----------+------+\n",
            "\n",
            "\n",
            "b) Average salary:\n",
            "+-----------------+\n",
            "|   average_salary|\n",
            "+-----------------+\n",
            "|4614.285714285715|\n",
            "+-----------------+\n",
            "\n",
            "\n",
            "c) Number of unique departments:\n",
            "+------------------+\n",
            "|unique_departments|\n",
            "+------------------+\n",
            "|                 3|\n",
            "+------------------+\n",
            "\n",
            "\n",
            "d) Number of unique salary values:\n",
            "+-------------------+\n",
            "|unique_salary_count|\n",
            "+-------------------+\n",
            "|                  9|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Create a data frame containing today‚Äôs date, date 2022-01-31, date\n",
        "# 2021-03-22, date 2024-01-31, date 2023-11-11.\n",
        "# b) Store the date in the format MM-DD-YYYY.\n",
        "# c) Display the dates in the format DD/MM/YYYY\n",
        "# d) Find the number of months between each of the dates and today‚Äôs date.\n",
        "\n",
        "# 2 a) Create data frame with a column that contains JSON string.\n",
        "# b) Convert the JSON string into Struct type or Map type.\n",
        "# c) Extract the Data from JSON and create them as new columns.\n",
        "# d) Convert MapType or Struct type to JSON string\n",
        "\n"
      ],
      "metadata": {
        "id": "VeU8qGg7DOpV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, current_date, to_date, date_format, lit, months_between\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip4_Q1\").getOrCreate()\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SOLUTION --- # a) and b) Create a DataFrame with dates stored in MM-DD-YYYY format\n",
        "date_data = [\n",
        "(\"01-31-2022\",), (\"03-22-2021\",), (\"01-31-2024\",), (\"11-11-2023\",)\n",
        "]\n",
        "# Add today's date to the list\n",
        "todays_date_str = spark.range(1).select(date_format(current_date(), \"MM-dd-yyyy\")).first()[0]\n",
        "date_data.append((todays_date_str,))\n",
        "date_df = spark.createDataFrame(date_data, [\"date_str_mmddyyyy\"])\n",
        "print(\"a) and b) DataFrame with dates in MM-DD-YYYY format:\")\n",
        "date_df.show()\n",
        "# To perform date operations, first convert the strings to a proper DateType\n",
        "df_with_dates = date_df.withColumn(\"date_obj\", to_date(col(\"date_str_mmddyyyy\"), \"MM-dd-yyyy\"))\n",
        "# c) Display the dates in the format DD/MM/YYYY\n",
        "print(\"\\nc) Dates displayed in DD/MM/YYYY format:\")\n",
        "df_formatted = df_with_dates.withColumn(\"date_str_ddmmyyyy\", date_format(col(\"date_obj\"), \"dd/MM/yyyy\"))\n",
        "df_formatted.select(\"date_str_mmddyyyy\", \"date_str_ddmmyyyy\").show()\n",
        "# d) Find the number of months between each date and today's date\n",
        "print(\"\\nd) Number of months between each date and today:\")\n",
        "df_months_between = df_with_dates.withColumn(\"months_from_today\", months_between(current_date(), col(\"date_obj\")))\n",
        "df_months_between.select(\"date_str_mmddyyyy\", \"months_from_today\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kviL-EnFEGBe",
        "outputId": "46f4ab00-a7ca-4d67-b55e-0ede27760a54"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "a) and b) DataFrame with dates in MM-DD-YYYY format:\n",
            "+-----------------+\n",
            "|date_str_mmddyyyy|\n",
            "+-----------------+\n",
            "|       01-31-2022|\n",
            "|       03-22-2021|\n",
            "|       01-31-2024|\n",
            "|       11-11-2023|\n",
            "|       10-07-2025|\n",
            "+-----------------+\n",
            "\n",
            "\n",
            "c) Dates displayed in DD/MM/YYYY format:\n",
            "+-----------------+-----------------+\n",
            "|date_str_mmddyyyy|date_str_ddmmyyyy|\n",
            "+-----------------+-----------------+\n",
            "|       01-31-2022|       31/01/2022|\n",
            "|       03-22-2021|       22/03/2021|\n",
            "|       01-31-2024|       31/01/2024|\n",
            "|       11-11-2023|       11/11/2023|\n",
            "|       10-07-2025|       07/10/2025|\n",
            "+-----------------+-----------------+\n",
            "\n",
            "\n",
            "d) Number of months between each date and today:\n",
            "+-----------------+-----------------+\n",
            "|date_str_mmddyyyy|months_from_today|\n",
            "+-----------------+-----------------+\n",
            "|       01-31-2022|      44.22580645|\n",
            "|       03-22-2021|      54.51612903|\n",
            "|       01-31-2024|      20.22580645|\n",
            "|       11-11-2023|      22.87096774|\n",
            "|       10-07-2025|              0.0|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. # Assuming the SparkSession 'spark' is already created from the previous question.\n",
        "from pyspark.sql.functions import from_json, to_json, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "# --- SOLUTION --- # a) Create DataFrame with a JSON string column\n",
        "json_data = [\n",
        "(1, '{\"name\":\"Alice\", \"city\":\"New York\"}'), (2, '{\"name\":\"Bob\", \"city\":\"Los Angeles\"}')\n",
        "]\n",
        "json_df = spark.createDataFrame(json_data, [\"id\", \"json_str\"])\n",
        "print(\"a) DataFrame with a JSON string column:\")\n",
        "json_df.show(truncate=False)\n",
        "# b) Convert the JSON string into Struct type\n",
        "# First, define the schema that matches the JSON structure\n",
        "json_schema = StructType([\n",
        "StructField(\"name\", StringType(), True), StructField(\"city\", StringType(), True)\n",
        "])\n",
        "df_with_struct = json_df.withColumn(\"parsed_struct\", from_json(col(\"json_str\"), json_schema))\n",
        "print(\"\\nb) DataFrame with JSON converted to a StructType column:\")\n",
        "df_with_struct.printSchema()\n",
        "df_with_struct.show(truncate=False)\n",
        "# c) Extract the Data from JSON and create them as new columns\n",
        "df_extracted = df_with_struct.withColumn(\"name\", col(\"parsed_struct.name\")) \\\n",
        ".withColumn(\"city\", col(\"parsed_struct.city\"))\n",
        "print(\"\\nc) DataFrame with JSON data extracted into new columns:\")\n",
        "df_extracted.select(\"id\", \"name\", \"city\").show()\n",
        "# d) Convert Struct type to JSON string\n",
        "df_converted_back = df_extracted.withColumn(\"new_json_str\", to_json(struct(\"name\", \"city\")))\n",
        "print(\"\\nd) DataFrame with columns converted back to a JSON string:\")\n",
        "df_converted_back.select(\"id\", \"new_json_str\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtvfTATCEX__",
        "outputId": "33478395-f684-4643-b308-06bddf5e41ae"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a) DataFrame with a JSON string column:\n",
            "+---+------------------------------------+\n",
            "|id |json_str                            |\n",
            "+---+------------------------------------+\n",
            "|1  |{\"name\":\"Alice\", \"city\":\"New York\"} |\n",
            "|2  |{\"name\":\"Bob\", \"city\":\"Los Angeles\"}|\n",
            "+---+------------------------------------+\n",
            "\n",
            "\n",
            "b) DataFrame with JSON converted to a StructType column:\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- json_str: string (nullable = true)\n",
            " |-- parsed_struct: struct (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            "\n",
            "+---+------------------------------------+------------------+\n",
            "|id |json_str                            |parsed_struct     |\n",
            "+---+------------------------------------+------------------+\n",
            "|1  |{\"name\":\"Alice\", \"city\":\"New York\"} |{Alice, New York} |\n",
            "|2  |{\"name\":\"Bob\", \"city\":\"Los Angeles\"}|{Bob, Los Angeles}|\n",
            "+---+------------------------------------+------------------+\n",
            "\n",
            "\n",
            "c) DataFrame with JSON data extracted into new columns:\n",
            "+---+-----+-----------+\n",
            "| id| name|       city|\n",
            "+---+-----+-----------+\n",
            "|  1|Alice|   New York|\n",
            "|  2|  Bob|Los Angeles|\n",
            "+---+-----+-----------+\n",
            "\n",
            "\n",
            "d) DataFrame with columns converted back to a JSON string:\n",
            "+---+-----------------------------------+\n",
            "|id |new_json_str                       |\n",
            "+---+-----------------------------------+\n",
            "|1  |{\"name\":\"Alice\",\"city\":\"New York\"} |\n",
            "|2  |{\"name\":\"Bob\",\"city\":\"Los Angeles\"}|\n",
            "+---+-----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a data frame containing today‚Äôs date, date 2022-01-31, date 2021-03-22, date 2024-01-31\n",
        "# Add 5 days to each date and display the result. Display the new dates after subtracting 10 days from each date.\n",
        "# For each date, display year, month, dayofweek, dayofmonth, dayofyear, next_day,weekofyear\n",
        "# 2. Refer to the employee.json file. Perform the following operations:\n",
        "# Print the names of employees above 25 years of age. Print the number of employees of different ages.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, to_date, date_add, date_sub, year, month, dayofweek, dayofmonth, dayofyear, next_day, weekofyear, current_date\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip5_Q1\").getOrCreate()\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SOLUTION --- # a) Create a DataFrame with dates\n",
        "# We create the DataFrame from string literals and convert them to DateType\n",
        "date_data = [\"2022-01-31\", \"2021-03-22\", \"2024-01-31\"]\n",
        "df_dates = spark.createDataFrame(date_data, \"string\").withColumnRenamed(\"value\", \"date_str\")\n",
        "df_dates = df_dates.union(spark.range(1).select(current_date().cast(\"string\").alias(\"date_str\"))) # Addtoday'sdate\n",
        "df = df_dates.withColumn(\"date\", to_date(col(\"date_str\")))\n",
        "print(\"a) Original DataFrame with dates:\")\n",
        "df.show()\n",
        "# b) Add 5 days to each date\n",
        "print(\"\\nb) Dates after adding 5 days:\")\n",
        "df_plus_5 = df.withColumn(\"date_plus_5\", date_add(col(\"date\"), 5))\n",
        "df_plus_5.show()\n",
        "# c) Display the new dates after subtracting 10 days from each date\n",
        "print(\"\\nc) Dates after subtracting 10 days:\")\n",
        "df_minus_10 = df.withColumn(\"date_minus_10\", date_sub(col(\"date\"), 10))\n",
        "df_minus_10.show()\n",
        "# d) Display various parts for each date\n",
        "print(\"\\nd) Various date parts:\")\n",
        "df_parts = df.withColumn(\"year\", year(col(\"date\"))) \\\n",
        ".withColumn(\"month\", month(col(\"date\"))) \\\n",
        ".withColumn(\"dayofweek\", dayofweek(col(\"date\"))) \\\n",
        ".withColumn(\"dayofmonth\", dayofmonth(col(\"date\"))) \\\n",
        ".withColumn(\"dayofyear\", dayofyear(col(\"date\"))) \\\n",
        ".withColumn(\"next_day\", next_day(col(\"date\"), \"Sunday\")) \\\n",
        ".withColumn(\"weekofyear\", weekofyear(col(\"date\")))\n",
        "df_parts.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f330AhdExMK",
        "outputId": "fbcd2cb9-3eca-4f1b-b951-b5039cf0e794"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "a) Original DataFrame with dates:\n",
            "+----------+----------+\n",
            "|  date_str|      date|\n",
            "+----------+----------+\n",
            "|2022-01-31|2022-01-31|\n",
            "|2021-03-22|2021-03-22|\n",
            "|2024-01-31|2024-01-31|\n",
            "|2025-10-07|2025-10-07|\n",
            "+----------+----------+\n",
            "\n",
            "\n",
            "b) Dates after adding 5 days:\n",
            "+----------+----------+-----------+\n",
            "|  date_str|      date|date_plus_5|\n",
            "+----------+----------+-----------+\n",
            "|2022-01-31|2022-01-31| 2022-02-05|\n",
            "|2021-03-22|2021-03-22| 2021-03-27|\n",
            "|2024-01-31|2024-01-31| 2024-02-05|\n",
            "|2025-10-07|2025-10-07| 2025-10-12|\n",
            "+----------+----------+-----------+\n",
            "\n",
            "\n",
            "c) Dates after subtracting 10 days:\n",
            "+----------+----------+-------------+\n",
            "|  date_str|      date|date_minus_10|\n",
            "+----------+----------+-------------+\n",
            "|2022-01-31|2022-01-31|   2022-01-21|\n",
            "|2021-03-22|2021-03-22|   2021-03-12|\n",
            "|2024-01-31|2024-01-31|   2024-01-21|\n",
            "|2025-10-07|2025-10-07|   2025-09-27|\n",
            "+----------+----------+-------------+\n",
            "\n",
            "\n",
            "d) Various date parts:\n",
            "+----------+----------+----+-----+---------+----------+---------+----------+----------+\n",
            "|  date_str|      date|year|month|dayofweek|dayofmonth|dayofyear|  next_day|weekofyear|\n",
            "+----------+----------+----+-----+---------+----------+---------+----------+----------+\n",
            "|2022-01-31|2022-01-31|2022|    1|        2|        31|       31|2022-02-06|         5|\n",
            "|2021-03-22|2021-03-22|2021|    3|        2|        22|       81|2021-03-28|        12|\n",
            "|2024-01-31|2024-01-31|2024|    1|        4|        31|       31|2024-02-04|         5|\n",
            "|2025-10-07|2025-10-07|2025|   10|        3|         7|      280|2025-10-12|        41|\n",
            "+----------+----------+----+-----+---------+----------+---------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. # Assuming the SparkSession 'spark' is already created from the previous question. from pyspark.sql.functions import col\n",
        "# --- SETUP --- # Create the employee.json file in the Colab environment\n",
        "json_content = \"\"\"\n",
        "{\"name\": \"Michael\", \"age\": 30}\n",
        "{\"name\": \"Andy\", \"age\": 24}\n",
        "{\"name\": \"Justin\", \"age\": 28}\n",
        "{\"name\": \"Berta\", \"age\": 35}\n",
        "{\"name\": \"David\", \"age\": 28} \"\"\"\n",
        "with open(\"employee.json\", \"w\") as f:\n",
        "  f.write(json_content)\n",
        "  print(\"--- employee.json file created successfully ---\\n\")\n",
        "# --- SOLUTION --- # Read the JSON file into a DataFrame\n",
        "emp_df = spark.read.json(\"employee.json\")\n",
        "print(\"Original Employee DataFrame:\")\n",
        "emp_df.show()\n",
        "# a) Print the names of employees above 25 years of age\n",
        "print(\"\\na) Names of employees older than 25:\")\n",
        "emp_df.filter(col(\"age\") > 25).select(\"name\").show()\n",
        "# b) Print the number of employees of different ages\n",
        "print(\"\\nb) Number of employees for each age:\")\n",
        "emp_df.groupBy(\"age\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wKF7_KXFOqt",
        "outputId": "cca5ef7e-6cf7-4746-b86f-47391ea806aa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- employee.json file created successfully ---\n",
            "\n",
            "Original Employee DataFrame:\n",
            "+---+-------+\n",
            "|age|   name|\n",
            "+---+-------+\n",
            "| 30|Michael|\n",
            "| 24|   Andy|\n",
            "| 28| Justin|\n",
            "| 35|  Berta|\n",
            "| 28|  David|\n",
            "+---+-------+\n",
            "\n",
            "\n",
            "a) Names of employees older than 25:\n",
            "+-------+\n",
            "|   name|\n",
            "+-------+\n",
            "|Michael|\n",
            "| Justin|\n",
            "|  Berta|\n",
            "|  David|\n",
            "+-------+\n",
            "\n",
            "\n",
            "b) Number of employees for each age:\n",
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 30|    1|\n",
            "| 24|    1|\n",
            "| 28|    2|\n",
            "| 35|    1|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two dataframes one for employee and other for dept. Perform :\n",
        "# a) Left anti join\n",
        "# b) Self join\n",
        "# c) Left semi join\n",
        "# 2 a) Create two case classes ‚Äì Student and Address\n",
        "# b) Create schema from these case classes\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip6_Q1\").getOrCreate()\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SETUP --- # Create the two DataFrames\n",
        "emp_data = [(1, \"Smith\", 10), (2, \"Rose\", 20), (3, \"Williams\", 10), (4, \"Jones\", 30), (5, \"Brown\", 50)]\n",
        "dept_data = [(\"Finance\", 10), (\"Marketing\", 20), (\"Sales\", 30), (\"IT\", 40)]\n",
        "emp_df = spark.createDataFrame(emp_data, [\"emp_id\", \"name\", \"dept_id\"])\n",
        "dept_df = spark.createDataFrame(dept_data, [\"dept_name\", \"dept_id\"])\n",
        "print(\"Employee DataFrame:\")\n",
        "emp_df.show()\n",
        "print(\"Department DataFrame:\")\n",
        "dept_df.show()\n",
        "# --- SOLUTION --- # a) Left Anti Join\n",
        "# This join returns only the rows from the left DataFrame that do not have a match in the right DataFrame.\n",
        "print(\"\\na) Left Anti Join (Employees in departments not in the dept table):\")\n",
        "emp_df.join(dept_df, on=\"dept_id\", how=\"left_anti\").show()\n",
        "# b) Self Join\n",
        "# This is joining a DataFrame to itself. You must use aliases to distinguish them. # Example: Find employees who have the same department ID.\n",
        "print(\"\\nb) Self Join (Find pairs of employees in the same department):\")\n",
        "df1 = emp_df.alias(\"df1\")\n",
        "df2 = emp_df.alias(\"df2\")\n",
        "# We add df1.emp_id < df2.emp_id to avoid duplicate pairs and self-joins.\n",
        "self_join_df = df1.join(df2, on=\"dept_id\") \\\n",
        ".where(col(\"df1.emp_id\") < col(\"df2.emp_id\")) \\\n",
        ".select(col(\"df1.name\").alias(\"emp1\"), col(\"df2.name\").alias(\"emp2\"), \"dept_id\")\n",
        "self_join_df.show()\n",
        "# c) Left Semi Join\n",
        "# This join is similar to an inner join, but it only returns the columns from the left DataFrame.\n",
        "print(\"\\nc) Left Semi Join (Employees in departments that exist in the dept table):\")\n",
        "emp_df.join(dept_df, on=\"dept_id\", how=\"left_semi\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-4wdaiUFxoh",
        "outputId": "f7db336d-0094-4009-fe65-48e82e6708bf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "Employee DataFrame:\n",
            "+------+--------+-------+\n",
            "|emp_id|    name|dept_id|\n",
            "+------+--------+-------+\n",
            "|     1|   Smith|     10|\n",
            "|     2|    Rose|     20|\n",
            "|     3|Williams|     10|\n",
            "|     4|   Jones|     30|\n",
            "|     5|   Brown|     50|\n",
            "+------+--------+-------+\n",
            "\n",
            "Department DataFrame:\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|  Finance|     10|\n",
            "|Marketing|     20|\n",
            "|    Sales|     30|\n",
            "|       IT|     40|\n",
            "+---------+-------+\n",
            "\n",
            "\n",
            "a) Left Anti Join (Employees in departments not in the dept table):\n",
            "+-------+------+-----+\n",
            "|dept_id|emp_id| name|\n",
            "+-------+------+-----+\n",
            "|     50|     5|Brown|\n",
            "+-------+------+-----+\n",
            "\n",
            "\n",
            "b) Self Join (Find pairs of employees in the same department):\n",
            "+-----+--------+-------+\n",
            "| emp1|    emp2|dept_id|\n",
            "+-----+--------+-------+\n",
            "|Smith|Williams|     10|\n",
            "+-----+--------+-------+\n",
            "\n",
            "\n",
            "c) Left Semi Join (Employees in departments that exist in the dept table):\n",
            "+-------+------+--------+\n",
            "|dept_id|emp_id|    name|\n",
            "+-------+------+--------+\n",
            "|     10|     1|   Smith|\n",
            "|     20|     2|    Rose|\n",
            "|     30|     4|   Jones|\n",
            "|     10|     3|Williams|\n",
            "+-------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Step 1: Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"PYQ1\").getOrCreate()\n",
        "\n",
        "# Step 2: Create Employee DataFrame\n",
        "employee_data = [\n",
        "    (1, \"John\", 101),\n",
        "    (2, \"Alice\", 102),\n",
        "    (3, \"Bob\", 103),\n",
        "    (4, \"Mary\", 104)\n",
        "]\n",
        "employee_df = spark.createDataFrame(employee_data, [\"EmpID\", \"EmpName\", \"DeptID\"])\n",
        "\n",
        "# Step 3: Create Department DataFrame\n",
        "dept_data = [\n",
        "    (101, \"HR\"),\n",
        "    (102, \"IT\"),\n",
        "    (105, \"Finance\")\n",
        "]\n",
        "dept_df = spark.createDataFrame(dept_data, [\"DeptID\", \"DeptName\"])\n",
        "\n",
        "# Display\n",
        "print(\"Employee DF\")\n",
        "employee_df.show()\n",
        "print(\"Department DF\")\n",
        "dept_df.show()\n",
        "\n",
        "# a) Left Anti Join\n",
        "left_anti_df = employee_df.join(dept_df, on=\"DeptID\", how=\"left_anti\")\n",
        "print(\"Left Anti Join (Employees without dept match):\")\n",
        "left_anti_df.show()\n",
        "\n",
        "# b) Self Join on Employee DF\n",
        "self_join_df = employee_df.alias(\"e1\").join(\n",
        "    employee_df.alias(\"e2\"),\n",
        "    col(\"e1.DeptID\") == col(\"e2.DeptID\")\n",
        ")\n",
        "print(\"Self Join on DeptID:\")\n",
        "self_join_df.show()\n",
        "\n",
        "# c) Left Semi Join\n",
        "left_semi_df = employee_df.join(dept_df, on=\"DeptID\", how=\"left_semi\")\n",
        "print(\"Left Semi Join (Employees with matching DeptID):\")\n",
        "left_semi_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaU5DRJXLWSm",
        "outputId": "d87b7123-d997-4bcc-dd25-0dc002a48f2a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employee DF\n",
            "+-----+-------+------+\n",
            "|EmpID|EmpName|DeptID|\n",
            "+-----+-------+------+\n",
            "|    1|   John|   101|\n",
            "|    2|  Alice|   102|\n",
            "|    3|    Bob|   103|\n",
            "|    4|   Mary|   104|\n",
            "+-----+-------+------+\n",
            "\n",
            "Department DF\n",
            "+------+--------+\n",
            "|DeptID|DeptName|\n",
            "+------+--------+\n",
            "|   101|      HR|\n",
            "|   102|      IT|\n",
            "|   105| Finance|\n",
            "+------+--------+\n",
            "\n",
            "Left Anti Join (Employees without dept match):\n",
            "+------+-----+-------+\n",
            "|DeptID|EmpID|EmpName|\n",
            "+------+-----+-------+\n",
            "|   104|    4|   Mary|\n",
            "|   103|    3|    Bob|\n",
            "+------+-----+-------+\n",
            "\n",
            "Self Join on DeptID:\n",
            "+-----+-------+------+-----+-------+------+\n",
            "|EmpID|EmpName|DeptID|EmpID|EmpName|DeptID|\n",
            "+-----+-------+------+-----+-------+------+\n",
            "|    1|   John|   101|    1|   John|   101|\n",
            "|    2|  Alice|   102|    2|  Alice|   102|\n",
            "|    4|   Mary|   104|    4|   Mary|   104|\n",
            "|    3|    Bob|   103|    3|    Bob|   103|\n",
            "+-----+-------+------+-----+-------+------+\n",
            "\n",
            "Left Semi Join (Employees with matching DeptID):\n",
            "+------+-----+-------+\n",
            "|DeptID|EmpID|EmpName|\n",
            "+------+-----+-------+\n",
            "|   101|    1|   John|\n",
            "|   102|    2|  Alice|\n",
            "+------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# a) Define Case Classes as Schema\n",
        "# Student\n",
        "student_schema = StructType([\n",
        "    StructField(\"StudentID\", IntegerType(), True),\n",
        "    StructField(\"StudentName\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Address\n",
        "address_schema = StructType([\n",
        "    StructField(\"StudentID\", IntegerType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"State\", StringType(), True)\n",
        "])\n",
        "\n",
        "# b) Create DataFrames using schema\n",
        "student_data = [(1, \"John\", 20), (2, \"Alice\", 21), (3, \"Bob\", 19)]\n",
        "address_data = [(1, \"Mumbai\", \"MH\"), (2, \"Delhi\", \"DL\"), (3, \"Kolkata\", \"WB\")]\n",
        "\n",
        "student_df = spark.createDataFrame(student_data, student_schema)\n",
        "address_df = spark.createDataFrame(address_data, address_schema)\n",
        "\n",
        "# Display\n",
        "print(\"Student DF\")\n",
        "student_df.show()\n",
        "print(\"Address DF\")\n",
        "address_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inAimXyDLYuX",
        "outputId": "32d5679d-1697-45aa-d785-e6f295601baf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student DF\n",
            "+---------+-----------+---+\n",
            "|StudentID|StudentName|Age|\n",
            "+---------+-----------+---+\n",
            "|        1|       John| 20|\n",
            "|        2|      Alice| 21|\n",
            "|        3|        Bob| 19|\n",
            "+---------+-----------+---+\n",
            "\n",
            "Address DF\n",
            "+---------+-------+-----+\n",
            "|StudentID|   City|State|\n",
            "+---------+-------+-----+\n",
            "|        1| Mumbai|   MH|\n",
            "|        2|  Delhi|   DL|\n",
            "|        3|Kolkata|   WB|\n",
            "+---------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. # Assuming the SparkSession 'spark' is already created from the previous question.\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "# --- EXPLANATION --- # \"Case classes\" are a feature of the Scala language. They provide a concise way to define\n",
        "# classes that are primarily used for holding data. Spark can automatically infer a schema fromtheminScala. #\n",
        "# The direct equivalent in Python is to use a standard Python class or, more commonly, # to define the schema explicitly using StructType and StructField.\n",
        "# This practical demonstrates the PySpark way of achieving the same goal.\n",
        "# --- SOLUTION --- # a) Equivalent of creating \"case classes\" # In Python, we can represent the data structure using standard classes or dictionaries.\n",
        "# Here, we'll represent the data as nested Row objects, which is a common pattern.\n",
        "student_data = [\n",
        "Row(name=\"John\", age=20, address=Row(city=\"New York\", zip_code=\"10001\")), Row(name=\"Jane\", age=22, address=Row(city=\"Los Angeles\", zip_code=\"90001\"))\n",
        "]\n",
        "# b) Create schema from the data structure\n",
        "# We explicitly define the schema to match our data structure.\n",
        "address_schema = StructType([\n",
        "StructField(\"city\", StringType(), True), StructField(\"zip_code\", StringType(), True)\n",
        "])\n",
        "student_schema = StructType([\n",
        "StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True), StructField(\"address\", address_schema, True)\n",
        "])\n",
        "print(\"b) Explicitly created schema:\")\n",
        "student_schema.prettyJson()\n",
        "# Now, create a DataFrame using this data and schema\n",
        "student_df = spark.createDataFrame(student_data, student_schema)\n",
        "print(\"\\nDataFrame created from the schema:\")\n",
        "student_df.show(truncate=False)\n",
        "student_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "6vmH294OGv_p",
        "outputId": "c750a37b-4494-4a12-c2a4-e19c5acac37b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b) Explicitly created schema:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'StructType' object has no attribute 'prettyJson'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1636478448.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m ])\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b) Explicitly created schema:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mstudent_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprettyJson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m# Now, create a DataFrame using this data and schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mstudent_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'StructType' object has no attribute 'prettyJson'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Create a data frame with data that follows the below given schema\n",
        "# emp_id, dept, properties (a structure containing salary and location) Return\n",
        "# the map keys from spark SQL for this data frame\n",
        "# 202 For the following employee data showing name, dept and salary, performthe\n",
        "# given operations:\n",
        "# Data: (\"James\", \"Sales\", 3000), (\"Michael\", \"Sales\", 4600), (\"Robert\", \"Sales\", 4100), (\"Maria\", \"Finance\", 3000), (\"James\", \"Sales\", 3000), (\"Scott\", \"Finance\", 3300), (\"Jen\", \"Finance\", 3900), (\"Jeff\", \"Marketing\", 3000), (\"Kumar\", \"Marketing\", 2000), (\"Saif\", \"Sales\", 4100), (Jason\", \"Sales\", 9000), (\"Alice\", \"Finance\", 3700), (\"Jenniffer\", \"Finance\", 8900), (\"Jenson\", \"Marketing\", 9000)\n",
        "# a) Create a data frame for the above data\n",
        "# b) Find the highest salary value\n",
        "# c) Find the lowest salary value\n",
        "# d) Find the standard deviation for the salary\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip7_Q1\").getOrCreate()\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- EXPLANATION --- # The question asks to \"Return the map keys\", but the `properties` column is a struct, not a map.\n",
        "# A struct has fixed fields (like keys), while a map can have arbitrary key-value pairs.\n",
        "# The correct interpretation is to access the fields of the struct, not \"map keys\".\n",
        " # --- SETUP --- # Create a DataFrame with a struct column\n",
        "data = [\n",
        "(1, \"Finance\", Row(salary=90000, location=\"New York\")), (2, \"Marketing\", Row(salary=80000, location=\"Chicago\")), (3, \"Sales\", Row(salary=120000, location=\"New York\"))\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"emp_id\", \"dept\", \"properties\"])\n",
        "print(\"Original DataFrame with Struct column:\")\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "# --- SOLUTION --- # Access the fields of the 'properties' struct\n",
        "print(\"\\nAccessing the fields ('keys') of the 'properties' struct:\")\n",
        "df.select(\n",
        "col(\"emp_id\"), col(\"properties.salary\"), col(\"properties.location\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyC8YvQsH61W",
        "outputId": "25c7bd6d-2990-4c05-9c23-c1f87e4bab98"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "Original DataFrame with Struct column:\n",
            "+------+---------+------------------+\n",
            "|emp_id|dept     |properties        |\n",
            "+------+---------+------------------+\n",
            "|1     |Finance  |{90000, New York} |\n",
            "|2     |Marketing|{80000, Chicago}  |\n",
            "|3     |Sales    |{120000, New York}|\n",
            "+------+---------+------------------+\n",
            "\n",
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- properties: struct (nullable = true)\n",
            " |    |-- salary: long (nullable = true)\n",
            " |    |-- location: string (nullable = true)\n",
            "\n",
            "\n",
            "Accessing the fields ('keys') of the 'properties' struct:\n",
            "+------+------+--------+\n",
            "|emp_id|salary|location|\n",
            "+------+------+--------+\n",
            "|     1| 90000|New York|\n",
            "|     2| 80000| Chicago|\n",
            "|     3|120000|New York|\n",
            "+------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the SparkSession 'spark' is already created from the previous question. from pyspark.sql.functions import max, min, stddev\n",
        "# --- SETUP --- # The provided data has a typo `(Jason\"`, which has been corrected to `(\"Jason\"`.\n",
        "employee_data = [\n",
        "(\"James\", \"Sales\", 3000), (\"Michael\", \"Sales\", 4600), (\"Robert\", \"Sales\", 4100), (\"Maria\", \"Finance\", 3000), (\"James\", \"Sales\", 3000), (\"Scott\", \"Finance\", 3300), (\"Jen\", \"Finance\", 3900), (\"Jeff\", \"Marketing\", 3000), (\"Kumar\", \"Marketing\", 2000), (\"Saif\", \"Sales\", 4100), (\"Jason\", \"Sales\", 9000), (\"Alice\", \"Finance\", 3700), (\"Jenniffer\", \"Finance\", 8900), (\"Jenson\", \"Marketing\", 9000)\n",
        "]\n",
        "columns = [\"name\", \"department\", \"salary\"]\n",
        "# --- SOLUTION --- # a) Create a data frame for the above data\n",
        "emp_df = spark.createDataFrame(employee_data, columns)\n",
        "print(\"a) Employee DataFrame:\")\n",
        "emp_df.show()\n",
        "# b) Find the highest salary value\n",
        "print(\"\\nb) Highest Salary:\")\n",
        "emp_df.select(max(\"salary\").alias(\"highest_salary\")).show()\n",
        "# c) Find the lowest salary value\n",
        "print(\"\\nc) Lowest Salary:\")\n",
        "emp_df.select(min(\"salary\").alias(\"lowest_salary\")).show()\n",
        "# d) Find the standard deviation for the salary\n",
        "print(\"\\nd) Standard Deviation of Salary:\")\n",
        "emp_df.select(stddev(\"salary\").alias(\"stddev_salary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mddu7sIL_wW",
        "outputId": "54c43b77-539f-4511-de57-44e8d173b33f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a) Employee DataFrame:\n",
            "+---------+----------+------+\n",
            "|     name|department|salary|\n",
            "+---------+----------+------+\n",
            "|    James|     Sales|  3000|\n",
            "|  Michael|     Sales|  4600|\n",
            "|   Robert|     Sales|  4100|\n",
            "|    Maria|   Finance|  3000|\n",
            "|    James|     Sales|  3000|\n",
            "|    Scott|   Finance|  3300|\n",
            "|      Jen|   Finance|  3900|\n",
            "|     Jeff| Marketing|  3000|\n",
            "|    Kumar| Marketing|  2000|\n",
            "|     Saif|     Sales|  4100|\n",
            "|    Jason|     Sales|  9000|\n",
            "|    Alice|   Finance|  3700|\n",
            "|Jenniffer|   Finance|  8900|\n",
            "|   Jenson| Marketing|  9000|\n",
            "+---------+----------+------+\n",
            "\n",
            "\n",
            "b) Highest Salary:\n",
            "+--------------+\n",
            "|highest_salary|\n",
            "+--------------+\n",
            "|          9000|\n",
            "+--------------+\n",
            "\n",
            "\n",
            "c) Lowest Salary:\n",
            "+-------------+\n",
            "|lowest_salary|\n",
            "+-------------+\n",
            "|         2000|\n",
            "+-------------+\n",
            "\n",
            "\n",
            "d) Standard Deviation of Salary:\n",
            "+-----------------+\n",
            "|    stddev_salary|\n",
            "+-----------------+\n",
            "|2444.729698085925|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Create a data frame with a nested array column. Perform the following\n",
        "# operations:\n",
        "# a) Flatten nested array\n",
        "# b) Explode nested array\n",
        "# c) Convert array of string to string column.\n",
        "\n",
        "# 2 a) Create data frame with a column that contains JSON string.\n",
        "# b) Convert the JSON string into Struct type or Map type.\n",
        "# c) Extract the Data from JSON and create them as new columns.\n",
        "# d) Convert MapType or Struct type to JSON string\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, flatten, explode, concat_ws\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip8_Q1\").getOrCreate()\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SETUP --- # Create the DataFrame with a nested array\n",
        "data = [\n",
        "(\"James\", [[\"Java\", \"Scala\", \"C++\"], [\"Spark\", \"Java\"]]), (\"Michael\", [[\"Spark\", \"Java\", \"C++\"], [\"Spark\", \"Java\"]]), (\"Robert\", [[\"CSharp\", \"VB\"], [\"Spark\", \"Python\"]])\n",
        "]\n",
        "schema = StructType([\n",
        "StructField(\"name\", StringType(), True), StructField(\"subjects\", ArrayType(ArrayType(StringType())), True)\n",
        "])\n",
        "df = spark.createDataFrame(data, schema)\n",
        "print(\"Original DataFrame with nested array:\")\n",
        "df.show(truncate=False)\n",
        "# --- SOLUTION ---\n",
        "# a) Flatten nested array\n",
        "print(\"\\na) Flattened nested array:\")\n",
        "df_a = df.withColumn(\"subjects_flat\", flatten(col(\"subjects\")))\n",
        "df_a.show(truncate=False)\n",
        "# b) Explode nested array\n",
        "print(\"\\nb) Exploded nested array to rows:\")\n",
        "# To get individual elements, you must flatten first.\n",
        "df_b = df_a.withColumn(\"subject\", explode(col(\"subjects_flat\")))\n",
        "df_b.show(truncate=False)\n",
        "# c) Convert array of string to string column\n",
        "print(\"\\nc) Converted array to a single string column:\")\n",
        "df_c = df_a.withColumn(\"subjects_string\", concat_ws(\", \", col(\"subjects_flat\")))\n",
        "df_c.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T3f_9KrMOCH",
        "outputId": "bb381fe5-b55f-4e19-f340-7a47063d8370"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "Original DataFrame with nested array:\n",
            "+-------+-----------------------------------+\n",
            "|name   |subjects                           |\n",
            "+-------+-----------------------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |\n",
            "+-------+-----------------------------------+\n",
            "\n",
            "\n",
            "a) Flattened nested array:\n",
            "+-------+-----------------------------------+-------------------------------+\n",
            "|name   |subjects                           |subjects_flat                  |\n",
            "+-------+-----------------------------------+-------------------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |\n",
            "+-------+-----------------------------------+-------------------------------+\n",
            "\n",
            "\n",
            "b) Exploded nested array to rows:\n",
            "+-------+-----------------------------------+-------------------------------+-------+\n",
            "|name   |subjects                           |subjects_flat                  |subject|\n",
            "+-------+-----------------------------------+-------------------------------+-------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|Java   |\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|Scala  |\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|C++    |\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|Spark  |\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|Java   |\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|Spark  |\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|Java   |\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|C++    |\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|Spark  |\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|Java   |\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |CSharp |\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |VB     |\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |Spark  |\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |Python |\n",
            "+-------+-----------------------------------+-------------------------------+-------+\n",
            "\n",
            "\n",
            "c) Converted array to a single string column:\n",
            "+-------+-----------------------------------+-------------------------------+-----------------------------+\n",
            "|name   |subjects                           |subjects_flat                  |subjects_string              |\n",
            "+-------+-----------------------------------+-------------------------------+-----------------------------+\n",
            "|James  |[[Java, Scala, C++], [Spark, Java]]|[Java, Scala, C++, Spark, Java]|Java, Scala, C++, Spark, Java|\n",
            "|Michael|[[Spark, Java, C++], [Spark, Java]]|[Spark, Java, C++, Spark, Java]|Spark, Java, C++, Spark, Java|\n",
            "|Robert |[[CSharp, VB], [Spark, Python]]    |[CSharp, VB, Spark, Python]    |CSharp, VB, Spark, Python    |\n",
            "+-------+-----------------------------------+-------------------------------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2. # Assuming the SparkSession 'spark' is already created from the previous question.\n",
        "from pyspark.sql.functions import from_json, to_json, col, struct\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "# --- SOLUTION --- # a) Create DataFrame with a JSON string column\n",
        "json_data = [\n",
        "(1, '{\"name\":\"Alice\", \"city\":\"New York\"}'), (2, '{\"name\":\"Bob\", \"city\":\"Los Angeles\"}')\n",
        "]\n",
        "json_df = spark.createDataFrame(json_data, [\"id\", \"json_str\"])\n",
        "print(\"a) DataFrame with a JSON string column:\")\n",
        "json_df.show(truncate=False)\n",
        "# b) Convert the JSON string into Struct type\n",
        "# Define the schema that matches the JSON structure\n",
        "json_schema = StructType([\n",
        "StructField(\"name\", StringType(), True), StructField(\"city\", StringType(), True)\n",
        "])\n",
        "df_with_struct = json_df.withColumn(\"parsed_struct\", from_json(col(\"json_str\"), json_schema))\n",
        "print(\"\\nb) DataFrame with JSON converted to a StructType column:\")\n",
        "df_with_struct.printSchema()\n",
        "df_with_struct.show(truncate=False)\n",
        "# c) Extract the Data from JSON and create them as new columns\n",
        "df_extracted = df_with_struct.withColumn(\"name\", col(\"parsed_struct.name\")) \\\n",
        ".withColumn(\"city\", col(\"parsed_struct.city\"))\n",
        "print(\"\\nc) DataFrame with JSON data extracted into new columns:\")\n",
        "df_extracted.select(\"id\", \"name\", \"city\").show()\n",
        "# d) Convert Struct type to JSON string\n",
        "df_converted_back = df_extracted.withColumn(\"new_json_str\", to_json(struct(\"name\", \"city\")))\n",
        "print(\"\\nd) DataFrame with columns converted back to a JSON string:\")\n",
        "df_converted_back.select(\"id\", \"new_json_str\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNlR-Tr_Mnfs",
        "outputId": "7df8e0d6-6de6-4311-9236-d15fccfacf10"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a) DataFrame with a JSON string column:\n",
            "+---+------------------------------------+\n",
            "|id |json_str                            |\n",
            "+---+------------------------------------+\n",
            "|1  |{\"name\":\"Alice\", \"city\":\"New York\"} |\n",
            "|2  |{\"name\":\"Bob\", \"city\":\"Los Angeles\"}|\n",
            "+---+------------------------------------+\n",
            "\n",
            "\n",
            "b) DataFrame with JSON converted to a StructType column:\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- json_str: string (nullable = true)\n",
            " |-- parsed_struct: struct (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            "\n",
            "+---+------------------------------------+------------------+\n",
            "|id |json_str                            |parsed_struct     |\n",
            "+---+------------------------------------+------------------+\n",
            "|1  |{\"name\":\"Alice\", \"city\":\"New York\"} |{Alice, New York} |\n",
            "|2  |{\"name\":\"Bob\", \"city\":\"Los Angeles\"}|{Bob, Los Angeles}|\n",
            "+---+------------------------------------+------------------+\n",
            "\n",
            "\n",
            "c) DataFrame with JSON data extracted into new columns:\n",
            "+---+-----+-----------+\n",
            "| id| name|       city|\n",
            "+---+-----+-----------+\n",
            "|  1|Alice|   New York|\n",
            "|  2|  Bob|Los Angeles|\n",
            "+---+-----+-----------+\n",
            "\n",
            "\n",
            "d) DataFrame with columns converted back to a JSON string:\n",
            "+---+-----------------------------------+\n",
            "|id |new_json_str                       |\n",
            "+---+-----------------------------------+\n",
            "|1  |{\"name\":\"Alice\",\"city\":\"New York\"} |\n",
            "|2  |{\"name\":\"Bob\",\"city\":\"Los Angeles\"}|\n",
            "+---+-----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark RDD using 5 different Functions\n",
        "\n",
        "# 2 Write example for following Spark RDD Actions:\n",
        "# a. aggregate b. treeAggregate\n",
        "# d. reduce e. collect\n",
        "# c. fold\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip9_Q1\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SETUP --- # Create a sample text file\n",
        "with open(\"sample_rdd.txt\", \"w\") as f:\n",
        "  f.write(\"line one\\n\")\n",
        "  f.write(\"line two\\n\")\n",
        "  f.write(\"line three\\n\")\n",
        "  print(\"--- Sample file for RDD creation created ---\\n\")\n",
        "# --- SOLUTION --- print(\"--- Creating RDDs using 5 different methods ---\\n\")\n",
        "# 1. Using sc.parallelize() on a list\n",
        "list_data = [1, 2, 3, 4, 5]\n",
        "rdd1 = sc.parallelize(list_data)\n",
        "print(\"1. RDD from a Python list (parallelize):\")\n",
        "print(rdd1.collect())\n",
        "# 2. Using sc.textFile() to read a text file\n",
        "rdd2 = sc.textFile(\"sample_rdd.txt\")\n",
        "print(\"\\n2. RDD from a text file (textFile):\")\n",
        "print(rdd2.collect())\n",
        "# 3. Using sc.range()\n",
        "rdd3 = sc.range(1, 6) # Creates an RDD with elements 1, 2, 3, 4, 5\n",
        "print(\"\\n3. RDD from a range (range):\")\n",
        "print(rdd3.collect())\n",
        "# 4. By transforming an existing RDD (e.g., using map)\n",
        "rdd4 = rdd1.map(lambda x: x * x)\n",
        "print(\"\\n4. RDD by transforming another RDD (map):\")\n",
        "print(rdd4.collect())\n",
        "# 5. From a DataFrame\n",
        "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2)], [\"letter\", \"number\"])\n",
        "rdd5 = df.rdd\n",
        "print(\"\\n5. RDD from a DataFrame (.rdd):\")\n",
        "print(rdd5.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0luX2MLAMxRw",
        "outputId": "de5ec9b3-bd45-426d-94ee-2c8b7cd38597"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "--- Sample file for RDD creation created ---\n",
            "\n",
            "1. RDD from a Python list (parallelize):\n",
            "[1, 2, 3, 4, 5]\n",
            "\n",
            "2. RDD from a text file (textFile):\n",
            "['line one', 'line two', 'line three']\n",
            "\n",
            "3. RDD from a range (range):\n",
            "[1, 2, 3, 4, 5]\n",
            "\n",
            "4. RDD by transforming another RDD (map):\n",
            "[1, 4, 9, 16, 25]\n",
            "\n",
            "5. RDD from a DataFrame (.rdd):\n",
            "[Row(letter='a', number=1), Row(letter='b', number=2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the SparkContext 'sc' is already created from the previous question.\n",
        "# --- SETUP ---\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "print(\"Using the following RDD for actions:\", rdd.collect())\n",
        "# --- SOLUTION --- # a) aggregate\n",
        "# Action: Sums elements and adds an initial value to each partition and then to the final result.\n",
        " # (sum_of_elements + initial_value * num_partitions) + initial_value\n",
        "# Here: (1+2+3+4+5) -> 15. Let's assume 2 partitions. # Partition 1: 1+2 -> 3. Partition 2: 3+4+5 -> 12.\n",
        "# Add initial value 10 to each partition sum: (3+10) + (12+10) = 45\n",
        "seqOp = (lambda x, y: x + y)\n",
        "combOp = (lambda x, y: x + y)\n",
        "agg_result = rdd.aggregate(0, seqOp, combOp) # Using 0 as initial value is same as reduce\n",
        "print(\"\\na) aggregate (sum):\", agg_result)\n",
        "# b) treeAggregate\n",
        "# Similar to aggregate but performs aggregation in a tree-like pattern, which is more efficient for large datasets.\n",
        "tree_agg_result = rdd.treeAggregate(0, seqOp, combOp)\n",
        "print(\"\\nb) treeAggregate (sum):\", tree_agg_result)\n",
        "# c) fold\n",
        "# Similar to reduce but takes a \"zero value\" to be used for the initial call in each partition.\n",
        "fold_result = rdd.fold(0, lambda x, y: x + y)\n",
        "print(\"\\nc) fold (sum):\", fold_result)\n",
        "# d) reduce\n",
        "# Aggregates the elements of the RDD using a specified commutative and associative binary operator.\n",
        "reduce_result = rdd.reduce(lambda x, y: x + y)\n",
        "print(\"\\nd) reduce (sum):\", reduce_result)\n",
        "# e) collect\n",
        "# Returns all the elements of the RDD as a list to the driver program.\n",
        "collect_result = rdd.collect()\n",
        "print(\"\\ne) collect:\", collect_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "610dPU0bNLhz",
        "outputId": "fba55e25-af40-454e-9735-59091303f846"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the following RDD for actions: [1, 2, 3, 4, 5]\n",
            "\n",
            "a) aggregate (sum): 15\n",
            "\n",
            "b) treeAggregate (sum): 15\n",
            "\n",
            "c) fold (sum): 15\n",
            "\n",
            "d) reduce (sum): 15\n",
            "\n",
            "e) collect: [1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Write example for following Spark RDD Actions:\n",
        "# a. count b. countApproxDistinct\n",
        "# c. first d. top e. Min\n",
        "# 2 Write Spark Pair RDD Functions.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip10_Q1\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SETUP ---\n",
        "rdd = sc.parallelize([2, 5, 1, 3, 4, 2, 5])\n",
        "print(\"Using the following RDD for actions:\", rdd.collect())\n",
        "# --- SOLUTION --- # a) count\n",
        "# Returns the number of elements in the RDD.\n",
        "count_result = rdd.count()\n",
        "print(\"\\na) count:\", count_result)\n",
        "# b) countApproxDistinct\n",
        "# Returns the approximate number of distinct elements. Useful for large datasets.\n",
        "approx_distinct_result = rdd.countApproxDistinct()\n",
        "print(\"\\nb) countApproxDistinct:\", approx_distinct_result)\n",
        "# Note: The exact distinct count is 5 (1, 2, 3, 4, 5)\n",
        "# c) first\n",
        "# Returns the first element of the RDD.\n",
        "first_result = rdd.first()\n",
        "print(\"\\nc) first:\", first_result)\n",
        "# d) top\n",
        "# Returns the top n elements from an RDD, ordered in descending order.\n",
        "top_3_result = rdd.top(3)\n",
        "print(\"\\nd) top(3):\", top_3_result)\n",
        "# e) min\n",
        "# Returns the minimum element of the RDD.\n",
        "min_result = rdd.min()\n",
        "print(\"\\ne) min:\", min_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM_6MO0QNu1o",
        "outputId": "e89eac51-264e-45b9-c3b2-39852f11d4d5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "Using the following RDD for actions: [2, 5, 1, 3, 4, 2, 5]\n",
            "\n",
            "a) count: 7\n",
            "\n",
            "b) countApproxDistinct: 5\n",
            "\n",
            "c) first: 2\n",
            "\n",
            "d) top(3): [5, 5, 4]\n",
            "\n",
            "e) min: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the SparkContext 'sc' is already created from the previous question.\n",
        "# --- SETUP --- # A Pair RDD is an RDD where each element is a key-value tuple.\n",
        " # Let's create a sample Pair RDD.\n",
        "data = [(\"apple\", 1), (\"banana\", 2), (\"apple\", 3), (\"orange\", 4), (\"banana\", 5)]\n",
        "pair_rdd = sc.parallelize(data)\n",
        "print(\"Using the following Pair RDD:\", pair_rdd.collect())\n",
        "# --- SOLUTION --- # Here are examples of common Pair RDD functions.\n",
        "# 1. reduceByKey()\n",
        "# Merges the values for each key using an associative and commutative reduce function.\n",
        "print(\"\\n1. reduceByKey (sum of values for each key):\")\n",
        "reduced_rdd = pair_rdd.reduceByKey(lambda a, b: a + b)\n",
        "print(reduced_rdd.collect())\n",
        "# 2. groupByKey()\n",
        "# Groups the values for each key in the RDD into a single sequence.\n",
        "print(\"\\n2. groupByKey:\")\n",
        "grouped_rdd = pair_rdd.groupByKey()\n",
        "# The result contains an iterable object, so we map it to a list for printing.\n",
        "print(grouped_rdd.mapValues(list).collect())\n",
        "# 3. sortByKey()\n",
        "# Sorts the RDD by key.\n",
        "print(\"\\n3. sortByKey (ascending):\")\n",
        "sorted_rdd = pair_rdd.sortByKey()\n",
        "print(sorted_rdd.collect())\n",
        "# 4. keys() and values()\n",
        "# Return an RDD of just the keys or just the values.\n",
        "print(\"\\n4. keys() and values():\")\n",
        "keys_rdd = pair_rdd.keys()\n",
        "values_rdd = pair_rdd.values()\n",
        "print(\"Keys:\", keys_rdd.collect())\n",
        "print(\"Values:\", values_rdd.collect())\n",
        "# 5. join()\n",
        "# Joins two Pair RDDs based on their keys.\n",
        "other_data = [(\"apple\", \"red\"), (\"orange\", \"orange\"), (\"grape\", \"purple\")]\n",
        "other_pair_rdd = sc.parallelize(other_data)\n",
        "print(\"\\n5. join:\")\n",
        "joined_rdd = pair_rdd.join(other_pair_rdd)\n",
        "print(joined_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfH3EUPJN-W5",
        "outputId": "fec2a4d5-4c2f-4898-ca0a-0647bbe94fdd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the following Pair RDD: [('apple', 1), ('banana', 2), ('apple', 3), ('orange', 4), ('banana', 5)]\n",
            "\n",
            "1. reduceByKey (sum of values for each key):\n",
            "[('apple', 4), ('banana', 7), ('orange', 4)]\n",
            "\n",
            "2. groupByKey:\n",
            "[('apple', [1, 3]), ('banana', [2, 5]), ('orange', [4])]\n",
            "\n",
            "3. sortByKey (ascending):\n",
            "[('apple', 1), ('apple', 3), ('banana', 2), ('banana', 5), ('orange', 4)]\n",
            "\n",
            "4. keys() and values():\n",
            "Keys: ['apple', 'banana', 'apple', 'orange', 'banana']\n",
            "Values: [1, 2, 3, 4, 5]\n",
            "\n",
            "5. join:\n",
            "[('apple', (1, 'red')), ('apple', (3, 'red')), ('orange', (4, 'orange'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get new dates by adding 4 days, and subtracting 7 days in below dates \"2020- 01-02\",\"2023-01-15\",\"2025-01-30\".\n",
        "# 2 Use the Operation Read CSV file on RDD with Scala operation\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, date_add, date_sub\n",
        "spark = SparkSession.builder.appName(\"PracticalExam_Slip11_Q1\").getOrCreate()\n",
        "print(\"--- Spark Session Created ---\\n\")\n",
        "# --- SETUP --- date_data = [\"2020-01-02\", \"2023-01-15\", \"2025-01-30\"]\n",
        "df = spark.createDataFrame(date_data, \"string\").withColumnRenamed(\"value\", \"date_str\")\n",
        "# Convert strings to DateType\n",
        "df = df.withColumn(\"date\", to_date(col(\"date_str\")))\n",
        "print(\"Original DataFrame with dates:\")\n",
        "df.show()\n",
        "# --- SOLUTION --- # Add 4 days to each date\n",
        "df_plus_4 = df.withColumn(\"date_plus_4_days\", date_add(col(\"date\"), 4))\n",
        "print(\"\\nDates after adding 4 days:\")\n",
        "df_plus_4.show()\n",
        "# Subtract 7 days from each date\n",
        "df_minus_7 = df.withColumn(\"date_minus_7_days\", date_sub(col(\"date\"), 7))\n",
        "print(\"\\nDates after subtracting 7 days:\")\n",
        "df_minus_7.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz7aBJu_OfLr",
        "outputId": "d5c41098-ba33-4620-d2da-083fcaf58478"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Spark Session Created ---\n",
            "\n",
            "Original DataFrame with dates:\n",
            "+----------+----------+\n",
            "|  date_str|      date|\n",
            "+----------+----------+\n",
            "|2022-01-31|2022-01-31|\n",
            "|2021-03-22|2021-03-22|\n",
            "|2024-01-31|2024-01-31|\n",
            "+----------+----------+\n",
            "\n",
            "\n",
            "Dates after adding 4 days:\n",
            "+----------+----------+----------------+\n",
            "|  date_str|      date|date_plus_4_days|\n",
            "+----------+----------+----------------+\n",
            "|2022-01-31|2022-01-31|      2022-02-04|\n",
            "|2021-03-22|2021-03-22|      2021-03-26|\n",
            "|2024-01-31|2024-01-31|      2024-02-04|\n",
            "+----------+----------+----------------+\n",
            "\n",
            "\n",
            "Dates after subtracting 7 days:\n",
            "+----------+----------+-----------------+\n",
            "|  date_str|      date|date_minus_7_days|\n",
            "+----------+----------+-----------------+\n",
            "|2022-01-31|2022-01-31|       2022-01-24|\n",
            "|2021-03-22|2021-03-22|       2021-03-15|\n",
            "|2024-01-31|2024-01-31|       2024-01-24|\n",
            "+----------+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the SparkContext 'sc' and SparkSession 'spark' are already created.\n",
        "# --- EXPLANATION --- # The question asks to use a \"Scala operation\". In PySpark, we use Python operations (like lambda functions).\n",
        "# The most common way to read a CSV is with a DataFrame, but to fulfill the \"on RDD\" requirement,\n",
        "# we will read the file as a text RDD and then parse it. # --- SETUP --- # Create a sample CSV file\n",
        "with open(\"student_data.csv\", \"w\") as f:\n",
        "  f.write(\"id,name,score\\n\")\n",
        "  f.write(\"1,Alice,85\\n\")\n",
        "  f.write(\"2,Bob,90\\n\")\n",
        "  f.write(\"3,Cathy,78\\n\")\n",
        "  print(\"--- student_data.csv created successfully ---\\n\")\n",
        "# --- SOLUTION --- # 1. Read the CSV file into a text RDD\n",
        "text_rdd = sc.textFile(\"student_data.csv\")\n",
        "print(\"RDD as raw text lines:\")\n",
        "print(text_rdd.collect())\n",
        "# 2. Get the header and filter it out\n",
        "header = text_rdd.first()\n",
        "data_rdd = text_rdd.filter(lambda line: line != header)\n",
        "print(\"\\nRDD after removing the header:\")\n",
        "print(data_rdd.collect())\n",
        "# 3. Use an RDD operation (map) to parse the data\n",
        "# This is the equivalent of a \"Scala operation\" in PySpark.\n",
        "parsed_rdd = data_rdd.map(lambda line: line.split(\",\"))\n",
        "# Convert score to an integer\n",
        "parsed_rdd = parsed_rdd.map(lambda parts: (int(parts[0]), parts[1], int(parts[2])))\n",
        "print(\"\\nRDD after parsing and transforming with a map operation:\")\n",
        "print(parsed_rdd.collect())\n",
        "# Example: Filter for scores above 80\n",
        "high_scores_rdd = parsed_rdd.filter(lambda x: x[2] > 80)\n",
        "print(\"\\nResult of another operation (filter for score > 80):\")\n",
        "print(high_scores_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeXQasmOOwWD",
        "outputId": "ad7e63d6-2fc7-4a03-e115-9d2c541ed439"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- student_data.csv created successfully ---\n",
            "\n",
            "RDD as raw text lines:\n",
            "['id,name,score', '1,Alice,85', '2,Bob,90', '3,Cathy,78']\n",
            "\n",
            "RDD after removing the header:\n",
            "['1,Alice,85', '2,Bob,90', '3,Cathy,78']\n",
            "\n",
            "RDD after parsing and transforming with a map operation:\n",
            "[(1, 'Alice', 85), (2, 'Bob', 90), (3, 'Cathy', 78)]\n",
            "\n",
            "Result of another operation (filter for score > 80):\n",
            "[(1, 'Alice', 85), (2, 'Bob', 90)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3erCbxVO-3Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}