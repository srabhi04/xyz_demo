Cancel
Send
ChatGPT said:

Perfect ‚úÖ ‚Äî now we‚Äôre doing this exactly right, Abhishek üëè

Since your Spark practical exam will be conducted on Google Colab, you need a clean, reusable setup that works for all your Spark practicals (1 ‚Üí 10) ‚Äî including RDDs, DataFrames, SQL, MLlib, and file operations.

Let‚Äôs build that universal setup block first.
Just run this once per session at the top of your Colab notebook ‚Äî then you can perform all Spark-based practicals smoothly.

üîß üî• Spark + Hadoop Environment Setup for Google Colab
üß© STEP 1 ‚Äî Install Java, Spark & FindSpark
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!tar xf spark-3.5.0-bin-hadoop3.tgz
!pip install -q findspark pyspark

üß© STEP 2 ‚Äî Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"
os.environ["PYSPARK_PYTHON"] = "/usr/bin/python3"

üß© STEP 3 ‚Äî Initialize Spark
import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("Spark_Practicals_Colab") \
    .config("spark.ui.showConsoleProgress", "false") \
    .getOrCreate()

spark

üß© STEP 4 ‚Äî Import Common Libraries
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import LogisticRegression

üß© STEP 5 ‚Äî (For XML or Avro Support)

If your practical includes XML or Avro files, install these extra JARs:

# XML support
!wget -q https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.15.0/spark-xml_2.12-0.15.0.jar -P /content/spark-3.5.0-bin-hadoop3/jars/

# Avro support
!wget -q https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.5.0/spark-avro_2.12-3.5.0.jar -P /content/spark-3.5.0-bin-hadoop3/jars/


(Run only if you plan to use XML or Avro.)

üß© STEP 6 ‚Äî Check Spark Version & Test Session
print("üî• Spark Version:", spark.version)
df_test = spark.createDataFrame([(1, "Abhishek"), (2, "Ashwin")], ["ID", "Name"])
df_test.show()


If this shows output correctly, your environment is 100 % ready for:

RDDs

DataFrames

SQL

Aggregations

MLlib

File reads/writes (CSV, JSON, Parquet, Avro, XML)

üß≠ Optional (for Visualization / Testing)

You can also install Pandas + Matplotlib to quickly inspect Spark outputs:

!pip install -q pandas matplotlib

‚úÖ After this setup

You can directly continue running any of the following practicals:

RDD Operations

DataFrame Operations

Array & Map Operations

Aggregations

SQL Joins & Functions

SQL Core Functions

Data Source API

Streaming (JSON dir only)

MLlib

HDFS/S3 (simulated locally)



------------------------------------------
üß† 1Ô∏è‚É£ How Google Colab Saves Your Work

When you‚Äôre signed into Colab with your Gmail account, every notebook you create (the .ipynb file) is automatically saved in your Google Drive ‚Üí ‚ÄúColab Notebooks‚Äù folder.

üìÅ Path in Drive:
My Drive ‚Üí Colab Notebooks

Everything you type or run is autosaved every few seconds.

‚öôÔ∏è 2Ô∏è‚É£ How to Save Your Setup Permanently
ü™Ñ Step 1: Create a New Notebook

Go to https://colab.research.google.com/

Sign in with your Gmail ID.

Click on ‚ÄúNew Notebook‚Äù (top-right corner).

Rename it ‚Üí
üëâ Spark_Practicals_Setup.ipynb

ü™Ñ Step 2: Paste This Initial Setup (Full Block)
# üöÄ SPARK ENVIRONMENT SETUP (Run this once per Colab session)

# 1Ô∏è‚É£ Install Java, Spark & Findspark
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz
!tar xf spark-3.5.0-bin-hadoop3.tgz
!pip install -q findspark pyspark

# 2Ô∏è‚É£ Set Environment Variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.5.0-bin-hadoop3"
os.environ["PYSPARK_PYTHON"] = "/usr/bin/python3"

# 3Ô∏è‚É£ Initialize Spark
import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .appName("Spark_Practicals_Colab") \
    .config("spark.ui.showConsoleProgress", "false") \
    .getOrCreate()

# 4Ô∏è‚É£ Import Common Libraries
from pyspark.sql import Row
from pyspark.sql.types import *
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import LogisticRegression

print("‚úÖ Spark Setup Complete! Version:", spark.version)

ü™Ñ Step 3: Test It Once

Run all cells ‚Üí It should print something like:

‚úÖ Spark Setup Complete! Version: 3.5.0

ü™Ñ Step 4: Save the Notebook

Colab saves automatically, but you can also click:
File ‚Üí Save a copy in Drive

Confirm it appears in Google Drive ‚Üí Colab Notebooks

Now your setup is stored permanently in your Google Drive ‚Äî linked to your Gmail account.

üíæ 3Ô∏è‚É£ How to Use It During Exam

During your practical:

Open Google Colab

Click on ‚ÄúFile ‚Üí Open Notebook‚Äù

Go to Google Drive ‚Üí Colab Notebooks

Open your saved notebook (e.g., Spark_Practicals_Setup.ipynb)

Run setup cells

Then add new cells below for your actual practical questions (RDD, DataFrame, SQL, etc.)

üîÑ 4Ô∏è‚É£ Optional ‚Äî Keep Backup Copies

If you want to be extra safe:

Save a backup copy as .ipynb on your Google Drive and email it to yourself.

Or download it to your local PC:

File ‚Üí Download ‚Üí Download .ipynb file

‚ö° 5Ô∏è‚É£ Pro Tip

Create one master notebook:

Spark_Practicals_Master.ipynb


And add sections:

# 1. RDD Practicals
# 2. DataFrame Practicals
# 3. SQL Practicals
# 4. MLlib Practicals


So even if you get any 3 random questions, you‚Äôll already have all code ready to execute.